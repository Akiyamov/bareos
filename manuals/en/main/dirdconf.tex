
\chapter{Director Configuration}
\label{DirectorChapter}
\index[general]{Director!Configuring the}
\index[general]{Configuring the Director}

Of all the configuration files needed to run {\bf Bareos}, the Director's is
the most complicated, and the one that you will need to modify the most often
as you add clients or modify the FileSets.

For a general discussion of configuration files and resources including the
data types recognized by {\bf Bareos}. Please see the
\ilink{Configuration}{ConfigureChapter} chapter of this manual.

%\section{Director Resource Types}
\index[general]{Types!Director Resource}
\index[general]{Director Resource Types}

Director resource type may be one of the following:

Job, JobDefs, Client, Storage, Catalog, Schedule, FileSet, Pool, Director,  or
Messages. We present them here in the most logical order for defining them:

Note, everything revolves around a job and is tied to a job in one
way or another.

\begin{itemize}
\item
   \ilink{Director}{DirectorResource} -- to  define the Director's
   name and its access password used for authenticating the Console program.
   Only a single  Director resource definition may appear in the Director's
   configuration file.  If you have either {\bf /dev/random} or  {\bf bc} on your
   machine, Bareos will generate a random password during the configuration
   process, otherwise it will  be left blank.
\item
   \ilink{Job}{JobResource} -- to define the backup/restore Jobs
   and to tie together the Client, FileSet and Schedule resources to  be used
   for each Job. Normally, you will Jobs of different names corresponding
   to each client (i.e. one Job per client, but a different one with a different name
   for each client).
\item
   \ilink{JobDefs}{JobDefsResource} -- optional resource for
   providing defaults for Job resources.
\item
   \ilink{Schedule}{ScheduleResource} -- to define when a Job is to
   be automatically run by {\bf Bareos's} internal scheduler. You
   may have any number of Schedules, but each job will reference only
   one.
\item
   \ilink{FileSet}{FileSetResource} -- to define the set of files
   to be backed up for each Client. You may have any number of
   FileSets but each Job will reference only one.
\item
   \ilink{Client}{ClientResource2} -- to define what Client is to be
   backed up. You will generally have multiple Client definitions. Each
   Job will reference only a single client.
\item
   \ilink{Storage}{StorageResource2} -- to define on what physical
   device the Volumes should be mounted. You may have one or
   more Storage definitions.
\item
   \ilink{Pool}{PoolResource} -- to define the pool of Volumes
   that can be used for a particular Job. Most people use a
   single default Pool.  However, if you have a large number
   of clients or volumes, you may want to have multiple Pools.
   Pools allow you to restrict a Job (or a Client) to use
   only a particular set of Volumes.
\item
   \ilink{Catalog}{CatalogResource} -- to define in what database to
   keep the list of files and the Volume names where they are backed up.
   Most people only use a single catalog.  However, if you want to
   scale the Director to many clients, multiple catalogs can be helpful.
   Multiple catalogs require a bit more management because in general
   you must know what catalog contains what data.  Currently, all
   Pools are defined in each catalog.  This restriction will be removed
   in a later release.
\item
   \ilink{Messages}{MessagesChapter} -- to define where error and
   information messages are to be sent or logged. You may define
   multiple different message resources and hence direct particular
   classes of messages to different users or locations (files, ...).
\end{itemize}

\section{Director Resource}
\label{DirectorResource4}
\index[general]{Director Resource}
\index[general]{Resource!Director}

The Director resource defines the attributes of the Directors running on the
network. In the current implementation, there is only a single Director
resource, but the final design will contain multiple Directors to maintain
index and media database redundancy.

\begin{description}

\item [Director] \hfill \\
\index[dir]{Director}
Start of the Director resource. One and only one  director resource must be
supplied.

\item [Name = {\textless}name{\textgreater}] \hfill \\
\index[dir]{Name}
\index[dir]{Directive!Name}
The director name used by the system  administrator. This directive is
required.

\item [Description = {\textless}text{\textgreater}] \hfill \\
\index[dir]{Description}
\index[dir]{Directive!Description}
The text field contains a  description of the Director that will be displayed
in the  graphical user interface. This directive is optional.

\item [Password = {\textless}UA-password{\textgreater}] \hfill \\
\index[dir]{Password}
\index[dir]{Directive!Password}
Specifies the password that must be supplied for the default Bareos
Console to be authorized.  The same password must appear in the {\bf
Director} resource of the Console configuration file.  For added
security, the password is never passed across the network but instead a
challenge response hash code created with the password.  This directive
is required.  If you have either {\bf /dev/random} or {\bf bc} on your
machine, Bareos will generate a random password during the configuration
process, otherwise it will be left blank and you must manually supply
it.

The password is plain text.  It is not generated through any special
process but as noted above, it is better to use random text for
security reasons.

\item [Messages = {\textless}Messages-resource-name{\textgreater}] \hfill \\
\index[dir]{Messages}
\index[dir]{Directive!Messages}
The messages resource  specifies where to deliver Director messages that are
not associated  with a specific Job. Most messages are specific to a job and
will  be directed to the Messages resource specified by the job. However,
there are a few messages that can occur when no job is running.  This
directive is required.

\item [Working Directory = {\textless}Directory{\textgreater}] \hfill \\
\index[dir]{Working Directory}
\index[dir]{Directive!Working Directory}
This directive is optional and specifies a directory in which the Director
may put its status files. This directory should be used only  by Bareos but
may be shared by other Bareos daemons. However, please note, if this
directory is shared with other Bareos daemons (the File daemon and Storage
daemon), you must ensure that the {\bf Name} given to each daemon is
unique so that the temporary filenames used do not collide.
By default
the Bareos configure process creates unique daemon names by postfixing them
with -dir, -fd, and -sd.
Standard shell expansion of the {\bf
Directory}  is done when the configuration file is read so that values such
as {\bf \$HOME} will be properly expanded.

The working directory specified must already exist and be
readable and writable by the Bareos daemon referencing it.

% If you have specified a Director user and/or a Director group on your
% ./configure line with {\bf {-}{-}with-dir-user} and/or
% {\bf {-}{-}with-dir-group} the Working Directory owner and group will
% be set to those values.

\item [Pid Directory = {\textless}Directory{\textgreater}] \hfill \\
\index[dir]{Pid Directory}
\index[dir]{Directive!Pid Directory}
This directive  is optional and specifies a directory in which the Director
may put its process Id file. The process Id file is used to  shutdown
Bareos and to prevent multiple copies of  Bareos from running simultaneously.
Standard shell expansion of the {\bf Directory}  is done when the
configuration file is read so that values such  as {\bf \$HOME} will be
properly expanded.

The PID directory specified must already exist and be
readable and writable by the Bareos daemon referencing it.

Typically on Linux systems, you will set this to:  {\bf /var/run}. If you are
not installing Bareos in the  system directories, you can use the {\bf Working
Directory} as  defined above.

\item [Scripts Directory = {\textless}Directory{\textgreater}] \hfill \\
\index[dir]{Scripts Directory}
\index[dir]{Directive!Scripts Directory}
This directive is currently unused.

\item [QueryFile = {\textless}Path{\textgreater}] \hfill \\
\index[dir]{QueryFile}
\index[dir]{Directive!QueryFile}
This directive is required and specifies a directory and file in which
the Director can find the canned SQL statements for the {\bf Query}
command of the Console.

%Standard shell expansion of the {\bf Path} is
%done when the configuration file is read so that values such as {\bf
%\$HOME} will be properly expanded.

\item [Heartbeat Interval = {\textless}time-interval{\textgreater}] \hfill \\
\index[dir]{Heartbeat Interval}
\index[dir]{Directive!Heartbeat}
This directive is optional and if specified will cause the Director to
set a keepalive interval (heartbeat) in seconds on each of the sockets
it opens for the Client resource.  This value will override any
specified at the Director level.  It is implemented only on systems
that provide the {\bf setsockopt} TCP\_KEEPIDLE function (Linux, ...).
The default value is zero, which means no change is made to the socket.

\directive{dir}{Maximum Concurrent Jobs}{number}{}{1}{}
\label{DirMaxConJobs}
%\item [Maximum Concurrent Jobs = {\textless}number{\textgreater}] \hfill \\
\index[dir]{Maximum Concurrent Jobs}
\index[dir]{Directive!Maximum Concurrent Jobs}
\index[general]{Simultaneous Jobs}
\index[general]{Concurrent Jobs}
where {\textless}number{\textgreater}  is the maximum number of total Director Jobs that
should run  concurrently. The default is set to 1, but you may set it to a
larger number.

The Volume format becomes more complicated with
multiple simultaneous jobs, consequently, restores may take longer if
Bareos must sort through interleaved volume blocks from  multiple simultaneous
jobs. This can be avoided by having each simultaneous job write to
a different volume or  by using data spooling, which will first spool the data
to disk simultaneously, then write one spool file at a time to the volume
thus avoiding excessive interleaving of the different job blocks.

See also the section about \ilink{Concurrent Jobs}{ConcurrentJobs}.

\item [FD Connect Timeout = {\textless}time{\textgreater}] \hfill \\
\index[dir]{FD Connect Timeout}
\index[dir]{Directive!FD Connect Timeout}
where {\bf time} is the time that the Director should continue
attempting to contact the File daemon to start a job, and after which
the Director will cancel the job.  The default is 30 minutes.

\item [SD Connect Timeout = {\textless}time{\textgreater}] \hfill \\
\index[dir]{SD Connect Timeout}
\index[dir]{Directive!SD Connect Timeout}
where {\bf time} is the time that the Director should continue
attempting to contact the Storage daemon to start a job, and after which
the Director will cancel the job.  The default is 30 minutes.

\item [DirAddresses = {\textless}IP-address-specification{\textgreater}] \hfill \\
\index[dir]{DirAddresses}
\index[dir]{Address}
\index[general]{Address}
\index[dir]{Directive!DirAddresses}
Specify the ports and addresses on which the Director daemon will listen
for Bareos Console connections.  Probably the simplest way to explain
this is to show an example:

\footnotesize
\begin{verbatim}
 DirAddresses  = {
    ip = { addr = 1.2.3.4; port = 1205;}
    ipv4 = {
        addr = 1.2.3.4; port = http;}
    ipv6 = {
        addr = 1.2.3.4;
        port = 1205;
    }
    ip = {
        addr = 1.2.3.4
        port = 1205
    }
    ip = { addr = 1.2.3.4 }
    ip = { addr = 201:220:222::2 }
    ip = {
        addr = bluedot.thun.net
    }
}
\end{verbatim}
\normalsize

where ip, ip4, ip6, addr, and port are all keywords. Note, that  the address
can be specified as either a dotted quadruple, or  IPv6 colon notation, or as
a symbolic name (only in the ip specification).  Also, port can be specified
as a number or as the mnemonic value from  the /etc/services file.  If a port
is not specified, the default will be used. If an ip  section is specified,
the resolution can be made either by IPv4 or  IPv6. If ip4 is specified, then
only IPv4 resolutions will be permitted,  and likewise with ip6.

Please note that if you use the DirAddresses directive, you must
not use either a DirPort or a DirAddress directive in the same
resource.

\item [DirPort = {\textless}port-number{\textgreater}] \hfill \\
\index[dir]{DirPort}
\index[dir]{Directive!DirPort}
Specify the port (a positive  integer) on which the  Director daemon will
listen for Bareos Console connections.  This same port number must be
specified in the Director resource  of the Console configuration file. The
default is 9101, so  normally this directive need not be specified.  This
directive should not be used if you specify DirAddresses (N.B plural)
directive.

\item [DirAddress = {\textless}IP-Address{\textgreater}] \hfill \\
\index[dir]{DirAddress}
\index[dir]{Directive!DirAddress}
This directive is optional, but if it is specified, it will cause the
Director server (for the Console program) to bind to the specified
{\bf IP-Address}, which is either a domain name or an IP address specified as a
dotted quadruple in string or quoted string format.  If this directive is
not specified, the Director will bind to any available address (the
default).  Note, unlike the DirAddresses specification noted above, this
directive only permits a single address to be specified.  This directive
should not be used if you specify a DirAddresses (N.B. plural) directive.

\item [DirSourceAddress = {\textless}IP-Address{\textgreater}] \hfill \\
\index[fd]{DirSourceAddress}
\index[fd]{Directive!DirSourceAddress}
This record is optional, and if it is specified, it will cause the Director
server (when initiating connections to a storage or file daemon) to source
its connections from the specified address.  Only a single IP address may be
specified.  If this record is not specified, the Director server will source
its outgoing connections according to the system routing table (the default).

\item [Statistics Retention = {\textless}time{\textgreater}] \hfill \\
\index[dir]{StatisticsRetention}
\index[dir]{Directive!StatisticsRetention}
\label{PruneStatistics}
The \texttt{Statistics Retention} directive defines the length of time that
Bareos will keep statistics job records in the Catalog database after the
Job End time (in \texttt{JobHistory} table). When this time period expires,
and if user runs \texttt{prune stats} command, Bareos will prune (remove)
Job records that are older than the specified period.

Theses statistics records aren't use for restore purpose, but mainly for
capacity planning, billings, etc.
See chapter about \ilink{how to extract information from the catalog}{UseBareosCatalogToExtractInformationChapter}
 for additional information.

See the \ilink{ Configuration chapter}{Time} of this manual for additional
details of time specification.

The default is 5 years.

\item [VerId = {\textless}string{\textgreater}] \hfill \\
\index[dir]{Directive!VerId}
where  {\textless}string{\textgreater} is an identifier which can be used for support purpose.
This string is displayed using the \texttt{version} command.

\item [MaximumConsoleConnections = {\textless}number{\textgreater}] \hfill \\
\index[dir]{MaximumConsoleConnections}
\index[dir]{Directive!MaximumConsoleConnections}
\index[dir]{Console}
where {\textless}number{\textgreater}  is the maximum number of Console Connections that
could run  concurrently. The default is set to 20, but you may set it to a
larger number.

\item [Optimize For Size = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Optimize For Size}
\index[dir]{Directive!Optimize For Size}
If {\bf Yes} which is the default, this directive will use the optimizations
for memory size over speed. So it will try to use less memory which may lead
to a somewhat lower speed. Its currently mostly used for keeping all hardlinks
in memory.

\item [Optimize For Speed = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Optimize For Speed}
\index[dir]{Directive!Optimize For Speed}
If {\bf Yes} which is {\bf Not} the default, this directive will use the
optimizations for speed over the memory size. So it will try to use more memory
which lead to a somewhat higher speed. Its currently mostly used for keeping all
hardlinks in memory. Its relates to the {\bf Optimize For Size} option set either
one to {\bf Yes} as they are mutually exclusive.

\item [Key Encryption Key = {\textless}KeyEncryptionKey{\textgreater}] \hfill \\
\index[dir]{Key Encryption Key}
\index[dir]{Directive!Key Encryption Key}
This key is used to encrypt the Security Key that is exchanged between
the Director and the Storage Daemon for supporting Application Managed
Encryption (AME). For security reasons each Director should have a
different Key Encryption Key.

\item [NDMP Snooping = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{NDMP Snooping}
\index[dir]{Directive!NDMP Snooping}
This directive enables the Snooping and pretty printing of NDMP protocol
information in debugging mode.

\item [NDMP Loglevel = {\textless}level{\textgreater}] \hfill \\
\index[dir]{NDMP Loglevel}
\index[dir]{Directive!NDMP Loglevel}
This directive sets the loglevel for the NDMP protocol library.
\end{description}

The following is an example of a valid Director resource definition:

\footnotesize
\begin{verbatim}
Director {
  Name = HeadMan
  WorkingDirectory = "$HOME/bareos/bin/working"
  Password = UA_password
  PidDirectory = "$HOME/bareos/bin/working"
  QueryFile = "$HOME/bareos/bin/query.sql"
  Messages = Standard
}
\end{verbatim}
\normalsize

\section{Job Resource}
\label{JobResource}
\index[general]{Resource!Job}
\index[general]{Job Resource}

The Job resource defines a Job (Backup, Restore, ...) that Bareos must
perform. Each Job resource definition contains the name of a Client and
a FileSet to backup, the Schedule for the Job, where the data
are to be stored, and what media Pool can be used. In effect, each Job
resource must specify What, Where, How, and When or FileSet, Storage,
Backup/Restore/Level, and Schedule respectively. Note, the FileSet must
be specified for a restore job for historical reasons, but it is no longer used.

Only a single type ({\bf Backup}, {\bf Restore}, ...) can be specified for any
job. If you want to backup multiple FileSets on the same Client or multiple
Clients, you must define a Job for each one.

Note, you define only a single Job to do the Full, Differential, and
Incremental backups since the different backup levels are tied together by
a unique Job name.  Normally, you will have only one Job per Client, but
if a client has a really huge number of files (more than several million),
you might want to split it into to Jobs each with a different FileSet
covering only part of the total files.

Multiple Storage daemons are not currently supported for Jobs, so if
you do want to use multiple storage daemons, you will need to create
a different Job and ensure that for each Job that the combination of
Client and FileSet are unique.  The Client and FileSet are what Bareos
uses to restore a client, so if there are multiple Jobs with the same
Client and FileSet or multiple Storage daemons that are used, the
restore will not work.  This problem can be resolved by defining multiple
FileSet definitions (the names must be different, but the contents of
the FileSets may be the same).


\begin{description}

\item [Job]
\index[dir]{Job}
\index[dir]{Directive!Job}
Start of the Job resource. At least one Job  resource is required.

\item [Name = {\textless}name{\textgreater}] \hfill \\
\index[dir]{Name}
\index[dir]{Directive!Name}
The Job name. This name can be specified  on the {\bf Run} command in the
console program to start a job. If the  name contains spaces, it must be
specified between quotes. It is  generally a good idea to give your job the
same name as the Client  that it will backup. This permits easy
identification of jobs.

When the job actually runs, the unique Job Name will consist  of the name you
specify here followed by the date and time the  job was scheduled for
execution. This directive is required.

\item [Enabled = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
  \index[dir]{Enable}
  \index[dir]{Directive!Enable}
  This directive allows you to enable or disable automatic execution
  via the scheduler of a Job.

\item [Type = {\textless}job-type{\textgreater}] \hfill \\
\index[dir]{Type}
\index[dir]{Directive!Type}
The {\bf Type} directive specifies  the Job type, which may be one of the
following: {\bf Backup},  {\bf Restore}, {\bf Verify}, or {\bf Admin}. This
directive  is required. Within a particular Job Type, there are also Levels
as discussed in the next item.

\begin{description}

\item [Backup] \hfill \\
\index[dir]{Backup}
Run a backup Job. Normally you will  have at least one Backup job for each
client you want  to save. Normally, unless you turn off cataloging,  most all
the important statistics and data concerning  files backed up will be placed
in the catalog.

\item [Restore] \hfill \\
\index[dir]{Restore}
Run a restore Job.  Normally, you will specify only one Restore job
which acts as a sort of prototype that you will modify using the console
program in order to perform restores.  Although certain basic
information from a Restore job is saved in the catalog, it is very
minimal compared to the information stored for a Backup job -- for
example, no File database entries are generated since no Files are
saved.

{\bf Restore} jobs cannot be
automatically started by the scheduler as is the case for Backup, Verify
and Admin jobs. To restore files, you must use the {\bf restore} command
in the console.


\item [Verify] \hfill \\
\index[dir]{Verify}
Run a verify Job. In general, {\bf verify}  jobs permit you to compare the
contents of the catalog  to the file system, or to what was backed up. In
addition,  to verifying that a tape that was written can be read,  you can
also use {\bf verify} as a sort of tripwire  intrusion detection.

\item [Admin] \hfill \\
\index[dir]{Admin}
Run an admin Job. An {\bf Admin} job can  be used to periodically run catalog
pruning, if you  do not want to do it at the end of each {\bf Backup}  Job.
Although an Admin job is recorded in the  catalog, very little data is saved.
\end{description}

\item [Protocol = {\textless}protocolname{\textgreater}] \hfill \\
\index[dir]{Protocol}
\index[dir]{Directive!Protocol}
The backup protocol to use to run the Job. If not set it will default
to {\bf Native} currently the director understand the following protocols:
\begin{enumerate}
\item Native - The native Bareos protocol
\item NDMP - The NDMP protocol
\end{enumerate}

\item [Backup Format = {\textless}backup format{\textgreater}] \hfill \\
\index[dir]{Backup Format}
\index[dir]{Directive!Backup Format}
The backup format used for protocols which support multiple formats. A protocol
like NDMP supports different backup formats for instance:
\begin{enumerate}
\item Dump
\item Tar
\item SMTape
\end{enumerate}

\label{Level}
\item [Level = {\textless}job-level{\textgreater}] \hfill \\
\index[dir]{Level}
\index[dir]{Directive!Level}
The Level directive specifies the default Job level to be run.  Each
different Job Type (Backup, Restore, ...) has a different set of Levels
that can be specified.  The Level is normally overridden by a different
value that is specified in the {\bf Schedule} resource.  This directive
is not required, but must be specified either by a {\bf Level} directive
or as an override specified in the {\bf Schedule} resource.

For a {\bf Backup} Job, the Level may be one of the  following:

\begin{description}

\item [Full] \hfill \\
\index[dir]{Full}
When the Level is set to Full all files in the FileSet whether or not
they have changed will be backed up.

\item [Incremental] \hfill \\
\index[dir]{Incremental}
When the Level is set to Incremental all files specified in the FileSet
that have changed since the last successful backup of the the same Job
using the same FileSet and Client, will be backed up.  If the Director
cannot find a previous valid Full backup then the job will be upgraded
into a Full backup.  When the Director looks for a valid backup record
in the catalog database, it looks for a previous Job with:

\begin{itemize}
\item The same Job name.
\item The same Client name.
\item The same FileSet (any change to the definition of  the FileSet such as
adding or deleting a file in the  Include or Exclude sections constitutes a
different FileSet.
\item The Job was a Full, Differential, or Incremental backup.
\item The Job terminated normally (i.e. did not fail or was not  canceled).
\item The Job started no longer ago than {\bf Max Full Interval}.
\end{itemize}

If all the above conditions do not hold, the Director will upgrade  the
Incremental to a Full save. Otherwise, the Incremental  backup will be
performed as requested.

The File daemon (Client) decides which files to backup for an
Incremental backup by comparing start time of the prior Job (Full,
Differential, or Incremental) against the time each file was last
"modified" (st\_mtime) and the time its attributes were last
"changed"(st\_ctime).  If the file was modified or its attributes
changed on or after this start time, it will then be backed up.

Some virus scanning software may change st\_ctime while
doing the scan.  For example, if the virus scanning program attempts to
reset the access time (st\_atime), which Bareos does not use, it will
cause st\_ctime to change and hence Bareos will backup the file during
an Incremental or Differential backup.  In the case of Sophos virus
scanning, you can prevent it from resetting the access time (st\_atime)
and hence changing st\_ctime by using the {\bf \verb:--:no-reset-atime}
option.  For other software, please see their manual.

When Bareos does an Incremental backup, all modified files that are
still on the system are backed up.  However, any file that has been
deleted since the last Full backup remains in the Bareos catalog,
which means that if between a Full save and the time you do a
restore, some files are deleted, those deleted files will also be
restored.  The deleted files will no longer appear in the catalog
after doing another Full save.

In addition, if you move a directory rather than copy it, the files in
it do not have their modification time (st\_mtime) or their attribute
change time (st\_ctime) changed.  As a consequence, those files will
probably not be backed up by an Incremental or Differential backup which
depend solely on these time stamps.  If you move a directory, and wish
it to be properly backed up, it is generally preferable to copy it, then
delete the original.

However, to manage deleted files or directories changes in the
catalog during an Incremental backup you can use \ilink{accurate mode}{accuratemode}.
This is quite memory consuming process.

\item [Differential] \hfill \\
\index[dir]{Differential}
When the Level is set to Differential
all files specified in the FileSet that have changed since the last
successful Full backup of the same Job will be backed up.
If the Director cannot find a
valid previous Full backup for the same Job, FileSet, and Client,
backup, then the Differential job will be upgraded into a Full backup.
When the Director looks for a valid Full backup record in the catalog
database, it looks for a previous Job with:

\begin{itemize}
\item The same Job name.
\item The same Client name.
\item The same FileSet (any change to the definition of  the FileSet such as
adding or deleting a file in the  Include or Exclude sections constitutes a
different FileSet.
\item The Job was a FULL backup.
\item The Job terminated normally (i.e. did not fail or was not  canceled).
\item The Job started no longer ago than {\bf Max Full Interval}.
\end{itemize}

If all the above conditions do not hold, the Director will  upgrade the
Differential to a Full save. Otherwise, the  Differential backup will be
performed as requested.

The File daemon (Client) decides which files to backup for a
differential backup by comparing the start time of the prior Full backup
Job against the time each file was last "modified" (st\_mtime) and the
time its attributes were last "changed" (st\_ctime).  If the file was
modified or its attributes were changed on or after this start time, it
will then be backed up.  The start time used is displayed after the {\bf
Since} on the Job report.  In rare cases, using the start time of the
prior backup may cause some files to be backed up twice, but it ensures
that no change is missed.  As with the Incremental option, you should
ensure that the clocks on your server and client are synchronized or as
close as possible to avoid the possibility of a file being skipped.
Note, on versions 1.33 or greater Bareos automatically makes the
necessary adjustments to the time between the server and the client so
that the times Bareos uses are synchronized.

When Bareos does a Differential backup, all modified files that are
still on the system are backed up.  However, any file that has been
deleted since the last Full backup remains in the Bareos catalog, which
means that if between a Full save and the time you do a restore, some
files are deleted, those deleted files will also be restored.  The
deleted files will no longer appear in the catalog after doing another
Full save.  However, to remove deleted files from the catalog during a
Differential backup is quite a time consuming process and not currently
implemented in Bareos.  It is, however, a planned future feature.

As noted above, if you move a directory rather than copy it, the
files in it do not have their modification time (st\_mtime) or
their attribute change time (st\_ctime) changed.  As a
consequence, those files will probably not be backed up by an
Incremental or Differential backup which depend solely on these
time stamps.  If you move a directory, and wish it to be
properly backed up, it is generally preferable to copy it, then
delete the original. Alternatively, you can move the directory, then
use the {\bf touch} program to update the timestamps.

%% TODO: merge this with incremental
However, to manage deleted files or directories changes in the
catalog during an Differential backup you can use \ilink{accurate mode}{accuratemode}.
This is quite memory consuming process. See  for more details.

Every once and a while, someone asks why we need Differential
backups as long as Incremental backups pickup all changed files.
There are possibly many answers to this question, but the one
that is the most important for me is that a Differential backup
effectively merges
all the Incremental and Differential backups since the last Full backup
into a single Differential backup.  This has two effects: 1.  It gives
some redundancy since the old backups could be used if the merged backup
cannot be read.  2.  More importantly, it reduces the number of Volumes
that are needed to do a restore effectively eliminating the need to read
all the volumes on which the preceding Incremental and Differential
backups since the last Full are done.

\end{description}

For a {\bf Restore} Job, no level needs to be specified.

For a {\bf Verify} Job, the Level may be one of the  following:

\begin{description}

\item [InitCatalog] \hfill \\
\index[dir]{InitCatalog}
does a scan of the specified {\bf FileSet} and stores the file
attributes in the Catalog database.  Since no file data is saved, you
might ask why you would want to do this.  It turns out to be a very
simple and easy way to have a {\bf Tripwire} like feature using {\bf
Bareos}.  In other words, it allows you to save the state of a set of
files defined by the {\bf FileSet} and later check to see if those files
have been modified or deleted and if any new files have been added.
This can be used to detect system intrusion.  Typically you would
specify a {\bf FileSet} that contains the set of system files that
should not change (e.g.  /sbin, /boot, /lib, /bin, ...).  Normally, you
run the {\bf InitCatalog} level verify one time when your system is
first setup, and then once again after each modification (upgrade) to
your system.  Thereafter, when your want to check the state of your
system files, you use a {\bf Verify} {\bf level = Catalog}.  This
compares the results of your {\bf InitCatalog} with the current state of
the files.

\item [Catalog] \hfill \\
\index[dir]{Catalog}
Compares the current state of the files against the state previously
saved during an {\bf InitCatalog}.  Any discrepancies are reported.  The
items reported are determined by the {\bf verify} options specified on
the {\bf Include} directive in the specified {\bf FileSet} (see the {\bf
FileSet} resource below for more details).  Typically this command will
be run once a day (or night) to check for any changes to your system
files.

\warning{If you run two Verify Catalog jobs on the same client at
the same time, the results will certainly be incorrect.  This is because
Verify Catalog modifies the Catalog database while running in order to
track new files.}

\item [VolumeToCatalog] \hfill \\
\index[dir]{VolumeToCatalog}
This level causes Bareos to read the file attribute data written to the
Volume from the last backup Job for the job specified on the {\bf VerifyJob}
directive.  The file attribute data are compared to the
values saved in the Catalog database and any differences are reported.
This is similar to the {\bf DiskToCatalog} level except that instead of
comparing the disk file attributes to the catalog database, the
attribute data written to the Volume is read and compared to the catalog
database.  Although the attribute data including the signatures (MD5 or
SHA1) are compared, the actual file data is not compared (it is not in
the catalog).

\warning{If you run two Verify VolumeToCatalog jobs on the same
client at the same time, the results will certainly be incorrect.  This
is because the Verify VolumeToCatalog modifies the Catalog database
while running.}

\item [DiskToCatalog] \hfill \\
\index[dir]{DiskToCatalog}
This level causes Bareos to read the files as they currently are on
disk, and to compare the current file attributes with the attributes
saved in the catalog from the last backup for the job specified on the
{\bf VerifyJob} directive.  This level differs from the {\bf VolumeToCatalog}
level described above by the fact that it doesn't compare against a
previous Verify job but against a previous backup.  When you run this
level, you must supply the verify options on your Include statements.
Those options determine what attribute fields are compared.

This command can be very useful if you have disk problems because it
will compare the current state of your disk against the last successful
backup, which may be several jobs.

Note, the current implementation does not identify files that
have been deleted.
\end{description}

\xdirective{dir}{Accurate}{yes{\textbar}no}{}{no}{}{%
\label{dir-resource-accurate}%
In accurate mode, the File daemon knowns exactly which files were present
after the last backup. So it is able to handle deleted or renamed files.

When restoring a FileSet for a specified date (including "most
recent"), Bareos is able to restore exactly the files and
directories that existed at the time of the last backup prior to
that date including ensuring that deleted files are actually deleted,
and renamed directories are restored properly.

In this mode, the File daemon must keep data concerning all files in
memory.  So If you do not have sufficient memory, the backup may
either be terribly slow or fail.

%%   $$ memory = \sum_{i=1}^{n}(strlen(path_i + file_i) + sizeof(CurFile))$$

For 500.000 files (a typical desktop linux system), it will require
approximately 64 Megabytes of RAM on your File daemon to hold the
required information.
}

\item [Verify Job = {\textless}Job-Resource-Name{\textgreater}] \hfill \\
\index[dir]{Verify Job}
\index[dir]{Directive!Verify Job}
If you run a verify job without this directive, the last job run will be
compared with the catalog, which means that you must immediately follow
a backup by a verify command.  If you specify a {\bf Verify Job} Bareos
will find the last job with that name that ran.  This permits you to run
all your backups, then run Verify jobs on those that you wish to be
verified (most often a {\bf VolumeToCatalog}) so that the tape just
written is re-read.

\item [Catalog = {\textless}Catalog-resource-name{\textgreater}] \hfill \\
\index[dir]{Catalog}
\index[dir]{Directive!Catalog}
This specifies the name of the catalog resource to be used for this Job.
When a catalog is defined in a Job it will override the definition in
the client. (This keyword was introduced in Bareos 13.4.0)

\item [JobDefs = {\textless}JobDefs-Resource-Name{\textgreater}] \hfill \\
\index[dir]{JobDefs}
\index[dir]{Directive!JobDefs}
If a JobDefs-Resource-Name is specified, all the values contained in the
named JobDefs resource will be used as the defaults for the current Job.
Any value that you explicitly define in the current Job resource, will
override any defaults specified in the JobDefs resource.  The use of
this directive permits writing much more compact Job resources where the
bulk of the directives are defined in one or more JobDefs.  This is
particularly useful if you have many similar Jobs but with minor
variations such as different Clients.  A simple example of the use of
JobDefs is provided in the default bareos-dir.conf file.

\item [Bootstrap = {\textless}bootstrap-file{\textgreater}] \hfill \\
\index[dir]{Bootstrap}
\index[dir]{Directive!Bootstrap}
The Bootstrap directive specifies a bootstrap file that, if provided,
will be used during {\bf Restore} Jobs and is ignored in other Job
types.  The {\bf bootstrap} file contains the list of tapes to be used
in a restore Job as well as which files are to be restored.
Specification of this directive is optional, and if specified, it is
used only for a restore job.  In addition, when running a Restore job
from the console, this value can be changed.

If you use the {\bf Restore} command in the Console program, to start a
restore job, the {\bf bootstrap} file will be created automatically from
the files you select to be restored.

For additional details of the {\bf bootstrap} file, please see
\ilink{Restoring Files with the Bootstrap File}{BootstrapChapter} chapter
of this manual.

\label{writebootstrap}
\item [Write Bootstrap =  {\textless}bootstrap-file-specification{\textgreater}] \hfill \\
\index[dir]{Write Bootstrap}
\index[dir]{Directive!Write Bootstrap}
The {\bf writebootstrap} directive specifies a file name where Bareos
will write a {\bf bootstrap} file for each Backup job run.  This
directive applies only to Backup Jobs.  If the Backup job is a Full
save, Bareos will erase any current contents of the specified file
before writing the bootstrap records.  If the Job is an Incremental
or Differential
save, Bareos will append the current bootstrap record to the end of the
file.

Using this feature, permits you to constantly have a bootstrap file that
can recover the current state of your system.  Normally, the file
specified should be a mounted drive on another machine, so that if your
hard disk is lost, you will immediately have a bootstrap record
available.  Alternatively, you should copy the bootstrap file to another
machine after it is updated. Note, it is a good idea to write a separate
bootstrap file for each Job backed up including the job that backs up
your catalog database.

If the {\bf bootstrap-file-specification} begins with a vertical bar
(\verb+|+), Bareos will use the specification as the name of a program to which
it will pipe the bootstrap record.  It could for example be a shell
script that emails you the bootstrap record.

Before opening the file or executing the
specified command, Bareos performs
\ilink{character substitution}{character substitution} like in RunScript
directive. To automatically manage your bootstrap files, you can use
this in your {\bf JobDefs} resources:
\begin{verbatim}
JobDefs {
   Write Bootstrap = "%c_%n.bsr"
   ...
}
\end{verbatim}

For more details on using this file, please see the chapter entitled
\ilink{The Bootstrap File}{BootstrapChapter} of this manual.

\item [Client = {\textless}client-resource-name{\textgreater}] \hfill \\
\index[dir]{Client}
\index[dir]{Directive!Client}
The Client directive  specifies the Client (File daemon) that will be used in
the  current Job. Only a single Client may be specified in any one Job.  The
Client runs on the machine to be backed up,  and sends the requested files to
the Storage daemon for backup,  or receives them when restoring. For
additional details, see the
\ilink{Client Resource section}{ClientResource2} of this chapter.
This directive is required (For versions before 13.3.0 for all Jobtypes
and for versions after that for all Jobtypes but Copy and Migrate).

\item [FileSet = {\textless}FileSet-resource-name{\textgreater}] \hfill \\
\index[dir]{FileSet}
\index[dir]{Directive!FileSet}
The FileSet directive specifies the FileSet that will be used in the
current Job.  The FileSet specifies which directories (or files) are to
be backed up, and what options to use (e.g.  compression, ...).  Only a
single FileSet resource may be specified in any one Job.  For additional
details, see the \ilink{FileSet Resource section}{FileSetResource} of
this chapter.
This directive is required (For versions before 13.3.0 for all Jobtypes
and for versions after that for all Jobtypes but Copy and Migrate).

\item [Base = {\textless}job-resource-name, ...{\textgreater}] \hfill \\
\index[dir]{Base}
\index[dir]{Directive!Base}
The Base directive permits to specify the list of jobs that will be used during
Full backup as base. This directive is optional. See the \ilink{Base Job
chapter}{basejobs} for more information.

\item [Messages = {\textless}messages-resource-name{\textgreater}] \hfill \\
\index[dir]{Messages}
\index[dir]{Directive!Messages}
The Messages directive defines what Messages resource should be used for
this job, and thus how and where the various messages are to be
delivered.  For example, you can direct some messages to a log file, and
others can be sent by email.  For additional details, see the
\ilink{Messages Resource}{MessagesChapter} Chapter of this manual.  This
directive is required.

\item [Pool = {\textless}pool-resource-name{\textgreater}] \hfill \\
\index[dir]{Pool}
\index[dir]{Directive!Pool}
The Pool directive defines the pool of Volumes where your data can be
backed up.  Many Bareos installations will use only the {\bf Default}
pool.  However, if you want to specify a different set of Volumes for
different Clients or different Jobs, you will probably want to use
Pools.  For additional details, see the \ilink{Pool Resource
section}{PoolResource} of this chapter.  This directive is required.

\item [Full Backup Pool = {\textless}pool-resource-name{\textgreater}] \hfill \\
\index[dir]{Full Backup Pool}
\index[dir]{Directive!Full Backup Pool}
The {\bf Full Backup Pool} specifies a Pool to be used for Full backups.
It will override any Pool specification during a Full backup.  This
directive is optional.

\item [Differential Backup Pool = {\textless}pool-resource-name{\textgreater}] \hfill \\
\index[dir]{Differential Backup Pool}
\index[dir]{Directive!Differential Backup Pool}
The {\bf Differential Backup Pool} specifies a Pool to be used for
Differential backups.  It will override any Pool specification during a
Differential backup.  This directive is optional.

\item [Incremental Backup Pool = {\textless}pool-resource-name{\textgreater}] \hfill \\
\index[dir]{Incremental Backup Pool}
\index[dir]{Directive!Incremental Backup Pool}
The {\bf Incremental Backup Pool} specifies a Pool to be used for
Incremental backups.  It will override any Pool specification during an
Incremental backup.  This directive is optional.

\item [Next Pool = {\textless}pool-resource-name{\textgreater}] \hfill \\
\index[dir]{Next Pool}
\index[dir]{Directive!Next Pool}
A Next Pool override used for Migration/Copy and Virtual Backup Jobs.

\item [Schedule = {\textless}schedule-name{\textgreater}] \hfill \\
\index[dir]{Schedule}
\index[dir]{Directive!Schedule}
The Schedule directive defines what schedule is to be used for the Job.
The schedule in turn determines when the Job will be automatically
started and what Job level (i.e.  Full, Incremental, ...) is to be run.
This directive is optional, and if left out, the Job can only be started
manually using the Console program.  Although you may specify only a
single Schedule resource for any one job, the Schedule resource may
contain multiple {\bf Run} directives, which allow you to run the Job at
many different times, and each {\bf run} directive permits overriding
the default Job Level Pool, Storage, and Messages resources.  This gives
considerable flexibility in what can be done with a single Job.  For
additional details, see the \ilink{Schedule Resource
Chapter}{ScheduleResource} of this manual.

\item [Storage = {\textless}storage-resource-name{\textgreater}] \hfill \\
\index[dir]{Storage}
\index[dir]{Directive!Storage}
The Storage directive defines the name of the storage services where you
want to backup the FileSet data.  For additional details, see the
\ilink{Storage Resource Chapter}{StorageResource2} of this manual.
The Storage resource may also be specified in the Job's Pool resource,
in which case the value in the Pool resource overrides any value
in the Job. This Storage resource definition is not required by either
the Job resource or in the Pool, but it must be specified in
one or the other, if not an error will result.

\item [Max Start Delay = {\textless}time{\textgreater}] \hfill \\
\index[dir]{Max Start Delay}
\index[dir]{Directive!Max Start Delay}
The time specifies the maximum delay between the scheduled time and the
actual start time for the Job.  For example, a job can be scheduled to
run at 1:00am, but because other jobs are running, it may wait to run.
If the delay is set to 3600 (one hour) and the job has not begun to run
by 2:00am, the job will be canceled.  This can be useful, for example,
to prevent jobs from running during day time hours.  The default is 0
which indicates no limit.

\item [Max Run Time = {\textless}time{\textgreater}] \hfill \\
\index[dir]{Max Run Time}
\index[dir]{Directive!Max Run Time}
The time specifies the maximum allowed time that a job may run, counted
from when the job starts, ({\bf not} necessarily the same as when the
job was scheduled).

By default, the the watchdog thread will kill any Job that has run more
than 6 days.  The maximum watchdog timeout is independent of MaxRunTime
and cannot be changed.

\item [Incremental|Differential Max Wait Time = {\textless}time{\textgreater}] \hfill \\
\index[dir]{Incremental Wait Run Time}
\index[dir]{Differential Wait Run Time}
\index[dir]{Directive!Differential Max Wait Time}
These directives have been deprecated in favor of
\texttt{Incremental|Differential Max Run Time}

\item [Incremental Max Run Time = {\textless}time{\textgreater}] \hfill \\
\index[dir]{Incremental Max Run Time}
\index[dir]{Directive!Incremental Max Run Time}
The time specifies the maximum allowed time that an Incremental backup job may
run, counted from when the job starts, ({\bf not} necessarily the same as when
the job was scheduled).

\item [Differential Max Wait Time = {\textless}time{\textgreater}] \hfill \\
\index[dir]{Differential Max Run Time}
\index[dir]{Directive!Differential Max Run Time}
The time specifies the maximum allowed time that a Differential backup job may
run, counted from when the job starts, ({\bf not} necessarily the same as when
the job was scheduled).

\item [Max Run Sched Time = {\textless}time{\textgreater}] \hfill \\
\index[dir]{Max Run Sched Time}
\index[dir]{Directive!Max Run Sched Time}
The time specifies the maximum allowed time that a job may run, counted from
when the job was scheduled. This can be useful to prevent jobs from running
during working hours. We can see it like \texttt{Max Start Delay + Max Run
Time}.

\item [Max Wait Time = {\textless}time{\textgreater}] \hfill \\
\index[dir]{Max Wait Time}
\index[dir]{Directive!Max Wait Time}
The time specifies the maximum allowed time that a job may block waiting
for a resource (such as waiting for a tape to be mounted, or waiting for
the storage or file daemons to perform their duties), counted from the
when the job starts, ({\bf not} necessarily the same as when the job was
scheduled).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=13cm]{\idir different_time}
  \caption{Job time control directives}
  \label{fig:differenttime}
\end{figure}

\item [Maximum Bandwidth = {\textless}speed{\textgreater}] \hfill \\
\index[dir]{Maximum Bandwidth}
\index[dir]{Directive!Maximum Bandwidth}
The speed parameter specifies the maximum allowed bandwidth that a job may
use. The speed parameter should be specified in k/s, kb/s, m/s or mb/s.

\item [Max Full Interval = {\textless}time{\textgreater}] \hfill \\
\index[dir]{Max Full Interval}
\index[dir]{Directive!Max Full Interval}
The time specifies the maximum allowed age (counting from start time) of
the most recent successful Full backup that is required in order to run
Incremental or Differential backup jobs. If the most recent Full backup
is older than this interval, Incremental and Differential backups will be
upgraded to Full backups automatically. If this directive is not present,
or specified as 0, then the age of the previous Full backup is not
considered.

\label{PreferMountedVolumes}
\item [Prefer Mounted Volumes = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Prefer Mounted Volumes}
\index[dir]{Directive!Prefer Mounted Volumes}
If the Prefer Mounted Volumes directive is set to {\bf yes} (default
yes), the Storage daemon is requested to select either an Autochanger or
a drive with a valid Volume already mounted in preference to a drive
that is not ready.  This means that all jobs will attempt to append
to the same Volume (providing the Volume is appropriate -- right Pool,
... for that job), unless you are using multiple pools.
If no drive with a suitable Volume is available, it
will select the first available drive.  Note, any Volume that has
been requested to be mounted, will be considered valid as a mounted
volume by another job.  This if multiple jobs start at the same time
and they all prefer mounted volumes, the first job will request the
mount, and the other jobs will use the same volume.

If the directive is set to {\bf no}, the Storage daemon will prefer
finding an unused drive, otherwise, each job started will append to the
same Volume (assuming the Pool is the same for all jobs).  Setting
Prefer Mounted Volumes to no can be useful for those sites
with multiple drive autochangers that prefer to maximize backup
throughput at the expense of using additional drives and Volumes.
This means that the job will prefer to use an unused drive rather
than use a drive that is already in use.

Despite the above, we recommend against setting this directive to
{\bf no} since
it tends to add a lot of swapping of Volumes between the different
drives and can easily lead to deadlock situations in the Storage
daemon. We will accept bug reports against it, but we cannot guarantee
that we will be able to fix the problem in a reasonable time.

A better alternative for using multiple drives is to use multiple
pools so that Bareos will be forced to mount Volumes from those Pools
on different drives.

\item [Prune Jobs = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Prune Jobs}
\index[dir]{Directive!Prune Jobs}
Normally, pruning of Jobs from the Catalog is specified on a Client by
Client basis in the Client resource with the {\bf AutoPrune} directive.
If this directive is specified (not normally) and the value is {\bf
yes}, it will override the value specified in the Client resource.  The
default is {\bf no}.


\item [Prune Files = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Prune Files}
\index[dir]{Directive!Prune Files}
Normally, pruning of Files from the Catalog is specified on a Client by
Client basis in the Client resource with the {\bf AutoPrune} directive.
If this directive is specified (not normally) and the value is {\bf
yes}, it will override the value specified in the Client resource.  The
default is {\bf no}.

\item [Prune Volumes = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Prune Volumes}
\index[dir]{Directive!Prune Volumes}
Normally, pruning of Volumes from the Catalog is specified on a Pool by
Pool basis in the Pool resource with the {\bf AutoPrune} directive.
Note, this is different from File and Job pruning which is done on a
Client by Client basis.  If this directive is specified (not normally)
and the value is {\bf yes}, it will override the value specified in the
Pool resource.  The default is {\bf no}.

\item [RunScript \{{\textless}body-of-runscript{\textgreater}\}] \hfill \\
\index[dir]{RunScript}
\index[dir]{Directive!Run Script}
The RunScript directive behaves like a resource in that it
requires opening and closing braces around a number of directives
that make up the body of the runscript.

The specified {\bf Command} (see below for details) is run as an external
program prior or after the current Job.  This is optional.  By default, the
program is executed on the Client side like in \texttt{ClientRunXXXJob}.

\textbf{Console} options are special commands that are sent to the director instead
of the OS. At this time, console command ouputs are redirected to log with
the jobid 0.

You can use following console command : \texttt{delete}, \texttt{disable},
\texttt{enable}, \texttt{estimate}, \texttt{list}, \texttt{llist},
\texttt{memory}, \texttt{prune}, \texttt{purge}, \texttt{reload},
\texttt{status}, \texttt{setdebug}, \texttt{show}, \texttt{time},
\texttt{trace}, \texttt{update}, \texttt{version}, \texttt{.client},
\texttt{.jobs}, \texttt{.pool}, \texttt{.storage}.  See console chapter for
more information. You need to specify needed information on command line, nothing
will be prompted. Example :

\begin{verbatim}
   Console = "prune files client=%c"
   Console = "update stats age=3"
\end{verbatim}

You can specify more than one Command/Console option per RunScript.

You can use following options may be specified in the body
of the runscript:\\

\begin{tabular}{|c|c|c|l}
\hline
Options         & Value  & Default & Information   \\
\hline
\hline
Runs On Success & Yes{\textbar}No & {\bf Yes} & Run command if JobStatus is successful\\
\hline
Runs On Failure & Yes{\textbar}No & {\bf No} & Run command if JobStatus isn't successful\\
\hline
Runs On Client  & Yes{\textbar}No & {\bf Yes} & Run command on client\\
\hline
Runs When       & Before{\textbar}After{\textbar}Always{\textbar}\textsl{AfterVSS} & {\bf Never} & When run commands\\
\hline
Fail Job On Error & Yes/No & {\bf Yes} & Fail job if script returns
                                          something different from 0 \\
\hline
Command          &       &          & Path to your script\\
\hline
Console          &       &          & Console command\\
\hline
\end{tabular}
   \\

Any output sent by the command to standard output will be included in the
Bareos job report.  The command string must be a valid program name or name
of a shell script.

In addition, the command string is parsed then fed to the OS,
which means that the path will be searched to execute your specified
command, but there is no shell interpretation, as a consequence, if you
invoke complicated commands or want any shell features such as redirection
or piping, you must call a shell script and do it inside that script.

Before submitting the specified command to the operating system, Bareos
performs character substitution of the following characters:

\label{character substitution}
\footnotesize
\begin{verbatim}
    %% = %
    %b = Job Bytes
    %c = Client's name
    %d = Daemon's name (Such as host-dir or host-fd)
    %D = Director's name (Also valid on file daemon)
    %e = Job Exit Status
    %f = Job FileSet (Only on director side)
    %F = Job Files
    %h = Client address
    %i = JobId
    %j = Unique Job id
    %l = Job Level
    %n = Job name
    %p = Pool name (Only on director side)
    %P = Daemon PID
    %s = Since time
    %t = Job type (Backup, ...)
    %v = Read Volume name(s) (Only on director side)
    %V = Write Volume name(s) (Only on director side)
    %w = Storage name (Only on director side)
    %x = Spooling enabled? ("yes" or "no")
\end{verbatim}
\normalsize

Some character substitutions are not available in all situations. The Job Exit
Status code \%e edits the following values:

\index[dir]{Exit Status}
\begin{itemize}
\item OK
\item Error
\item Fatal Error
\item Canceled
\item Differences
\item Unknown term code
\end{itemize}

   Thus if you edit it on a command line, you will need to enclose
   it within some sort of quotes.


You can use these following shortcuts:\\

\begin{tabular}{|c|c|c|c|c|c}
\hline
Keyword & RunsOnSuccess & RunsOnFailure  & FailJobOnError & Runs On Client & RunsWhen  \\
\hline
\hline
Run Before Job         &        &       & Yes     & No     & Before \\
\hline
Run After Job          &  Yes   &   No  &         & No     & After  \\
\hline
Run After Failed Job   &  No    &  Yes  &         & No     & After  \\
\hline
Client Run Before Job  &        &       & Yes     & Yes    & Before \\
\hline
Client Run After Job   &  Yes   &   No  &         & Yes    & After  \\
\hline
\end{tabular}

Examples:
\begin{verbatim}
RunScript {
    RunsWhen = Before
    FailJobOnError = No
    Command = "/etc/init.d/apache stop"
}

RunScript {
    RunsWhen = After
    RunsOnFailure = yes
    Command = "/etc/init.d/apache start"
}
\end{verbatim}

{\bf Notes about ClientRunBeforeJob}

For compatibility reasons, with this shortcut, the command is executed
directly when the client receive it. And if the command is in error, other
remote runscripts will be discarded. To be sure that all commands will be
sent and executed, you have to use RunScript syntax.

{\bf Special Windows Considerations}

You can run scripts just after snapshots initializations with
\textsl{AfterVSS} keyword.

In addition, for a Windows client, please take
note that you must ensure a correct path to your script.  The script or
program can be a .com, .exe or a .bat file.  If you just put the program
name in then Bareos will search using the same rules that cmd.exe uses
(current directory, Bareos bin directory, and PATH).  It will even try the
different extensions in the same order as cmd.exe.
The command can be anything that cmd.exe or command.com will recognize
as an executable file.

However, if you have slashes in the program name then Bareos figures you
are fully specifying the name, so you must also explicitly add the three
character extension.

The command is run in a Win32 environment, so Unix like commands will not
work unless you have installed and properly configured Cygwin in addition
to and separately from Bareos.

The System \%Path\% will be searched for the command.  (under the
environment variable dialog you have have both System Environment and
User Environment, we believe that only the System environment will be
available to bareos-fd, if it is running as a service.)

System environment variables can be referenced with \%var\% and
used as either part of the command name or arguments.

So if you have a script in the Bareos\\bin directory then the following lines
should work fine:

\footnotesize
\begin{verbatim}
        Client Run Before Job = systemstate
or
        Client Run Before Job = systemstate.bat
or
        Client Run Before Job = "systemstate"
or
        Client Run Before Job = "systemstate.bat"
or
        ClientRunBeforeJob = "\"C:/Program Files/Bareos/systemstate.bat\""
\end{verbatim}
\normalsize

The outer set of quotes is removed when the configuration file is parsed.
You need to escape the inner quotes so that they are there when the code
that parses the command line for execution runs so it can tell what the
program name is.

\footnotesize
\begin{verbatim}
ClientRunBeforeJob = "\"C:/Program Files/Software
     Vendor/Executable\" /arg1 /arg2 \"foo bar\""
\end{verbatim}
\normalsize

The special characters
\begin{verbatim}
&<>()@^|
\end{verbatim}
will need to be quoted,
if they are part of a filename or argument.

If someone is logged in, a blank "command" window running the commands
will be present during the execution of the command.

Some Suggestions from Phil Stracchino for running on Win32 machines with
the native Win32 File daemon:

\begin{enumerate}
\item You might want the ClientRunBeforeJob directive to specify a .bat
      file which runs the actual client-side commands, rather than trying
      to run (for example) regedit /e directly.
\item The batch file should explicitly 'exit 0' on successful completion.
\item The path to the batch file should be specified in Unix form:

ClientRunBeforeJob = "c:/bareos/bin/systemstate.bat"

rather than DOS/Windows form:

ClientRunBeforeJob =

"c:\textbackslash{}bareos\textbackslash{}bin\textbackslash{}systemstate.bat"
   INCORRECT
   \end{enumerate}

For Win32, please note that there are certain limitations:

ClientRunBeforeJob = "C:/Program Files/Bareos/bin/pre-exec.bat"

Lines like the above do not work because there are limitations of
cmd.exe that is used to execute the command.
Bareos prefixes the string you supply with {\bf cmd.exe /c }.  To test that
your command works you should type {\bf cmd /c "C:/Program Files/test.exe"} at a
cmd prompt and see what happens.  Once the command is correct insert a
backslash (\textbackslash{}) before each double quote ("), and
then put quotes around the whole thing when putting it in
the director's .conf file.  You either need to have only one set of quotes
or else use the short name and don't put quotes around the command path.

Below is the output from cmd's help as it relates to the command line
passed to the /c option.

If /C or /K is specified, then the remainder of the command line after
the switch is processed as a command line, where the following logic is
used to process quote (") characters:

\begin{enumerate}
\item
If all of the following conditions are met, then quote characters
on the command line are preserved:
\begin{itemize}
\item no /S switch.
\item exactly two quote characters.
\item no special characters between the two quote characters,
where special is one of:
\begin{verbatim}
&<>()@^|
\end{verbatim}
\item there are one or more whitespace characters between the
the two quote characters.
\item the string between the two quote characters is the name
of an executable file.
\end{itemize}

\item  Otherwise, old behavior is to see if the first character is
a quote character and if so, strip the leading character and
remove the last quote character on the command line, preserving
any text after the last quote character.
\end{enumerate}

The following example of the use of the Client Run Before Job directive was
submitted by a user:\\
You could write a shell script to back up a DB2 database to a FIFO. The shell
script is:

\footnotesize
\begin{verbatim}
 #!/bin/sh
 # ===== backupdb.sh
 DIR=/u01/mercuryd

 mkfifo $DIR/dbpipe
 db2 BACKUP DATABASE mercuryd TO $DIR/dbpipe WITHOUT PROMPTING &
 sleep 1
\end{verbatim}
\normalsize

The following line in the Job resource in the bareos-dir.conf file:
\footnotesize
\begin{verbatim}
 Client Run Before Job = "su - mercuryd -c \"/u01/mercuryd/backupdb.sh '%t'
'%l'\""
\end{verbatim}
\normalsize

When the job is run, you will get messages from the output of the script
stating that the backup has started. Even though the command being run is
backgrounded with \&, the job will block until the "db2 BACKUP DATABASE"
command, thus the backup stalls.

To remedy this situation, the "db2 BACKUP DATABASE" line should be changed to
the following:

\footnotesize
\begin{verbatim}
 db2 BACKUP DATABASE mercuryd TO $DIR/dbpipe WITHOUT PROMPTING > $DIR/backup.log
2>&1 < /dev/null &
\end{verbatim}
\normalsize

It is important to redirect the input and outputs of a backgrounded command to
/dev/null to prevent the script from blocking.

\item [Run Before Job = {\textless}command{\textgreater}] \hfill \\
\index[dir]{Run Before Job}
\index[dir]{Directive!Run Before Job}
\index[dir]{Directive!Run Before Job}
The specified {\bf command} is run as an external program prior to running the
current Job.  This directive is not required, but if it is defined, and if the
exit code of the program run is non-zero, the current Bareos job will be
canceled.

\begin{verbatim}
Run Before Job = "echo test"
\end{verbatim}
   it's equivalent to :
\begin{verbatim}
RunScript {
 Command = "echo test"
 RunsOnClient = No
 RunsWhen = Before
}
\end{verbatim}

Lutz Kittler has pointed out that using the RunBeforeJob directive can be a
simple way to modify your schedules during a holiday.  For example, suppose
that you normally do Full backups on Fridays, but Thursday and Friday are
holidays.  To avoid having to change tapes between Thursday and Friday when
no one is in the office, you can create a RunBeforeJob that returns a
non-zero status on Thursday and zero on all other days.  That way, the
Thursday job will not run, and on Friday the tape you inserted on Wednesday
before leaving will be used.

\item [Run After Job = {\textless}command{\textgreater}] \hfill \\
\index[dir]{Run After Job}
\index[dir]{Directive!Run After Job}
The specified {\bf command} is run as an external program if the current
job terminates normally (without error or without being canceled).  This
directive is not required.  If the exit code of the program run is
non-zero, Bareos will print a warning message.  Before submitting the
specified command to the operating system, Bareos performs character
substitution as described above for the {\bf RunScript} directive.

%An example of the use of this directive is given in the
%\ilink{Tips Chapter}{JobNotification} of this manual.

See the {\bf Run After Failed Job} if you
want to run a script after the job has terminated with any
non-normal status.

\item [Run After Failed Job = {\textless}command{\textgreater}] \hfill \\
\index[dir]{Run After Job}
\index[dir]{Directive!Run After Job}
The specified {\bf command} is run as an external program after the current
job terminates with any error status.  This directive is not required.  The
command string must be a valid program name or name of a shell script. If
the exit code of the program run is non-zero, Bareos will print a
warning message. Before submitting the specified command to the
operating system, Bareos performs character substitution as described above
for the {\bf RunScript} directive. Note, if you wish that your script
will run regardless of the exit status of the Job, you can use this :

\begin{verbatim}
RunScript {
 Command = "echo test"
 RunsWhen = After
 RunsOnFailure = yes
 RunsOnClient  = no
 RunsOnSuccess = yes    # default, you can drop this line
}
\end{verbatim}

%An example of the use of this directive is given in the
%\ilink{Tips Chapter}{JobNotification} of this manual.

\item [Client Run Before Job = {\textless}command{\textgreater}] \hfill \\
\index[dir]{Client Run Before Job}
\index[dir]{Directive!Client Run Before Job}
This directive is the same as {\bf Run Before Job} except that the
program is run on the client machine.  The same restrictions apply to
Unix systems as noted above for the {\bf RunScript}.

\item [Client Run After Job = {\textless}command{\textgreater}] \hfill \\
\index[dir]{Client Run After Job}
\index[dir]{Directive!Client Run After Job}
The specified {\bf command} is run on the client machine as soon
as data spooling is complete in order to allow restarting applications
on the client as soon as possible. .

Note, please see the notes above in {\bf RunScript}
concerning Windows clients.

\item [Rerun Failed Levels = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Rerun Failed Levels}
\index[dir]{Directive!Rerun Failed Levels}
If this directive is set to {\bf yes} (default no), and Bareos detects that
a previous job at a higher level (i.e.  Full or Differential) has failed,
the current job level will be upgraded to the higher level.  This is
particularly useful for Laptops where they may often be unreachable, and if
a prior Full save has failed, you wish the very next backup to be a Full
save rather than whatever level it is started as.

There are several points that must be taken into account when using this
directive: first, a failed job is defined as one that has not terminated
normally, which includes any running job of the same name (you need to
ensure that two jobs of the same name do not run simultaneously);
secondly, the {\bf Ignore FileSet Changes} directive is not considered
when checking for failed levels, which means that any FileSet change will
trigger a rerun.

\item [Spool Data = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Spool Data}
\index[dir]{Directive!Spool Data}

If this directive is set  to {\bf yes} (default no), the Storage daemon will
be requested  to spool the data for this Job to disk rather than write it
directly to the Volume (normally a tape).

Thus the data is written in large blocks to the Volume rather than small
blocks.  This directive is particularly useful when running multiple
simultaneous backups to tape.  Once all the data arrives or the spool
files' maximum sizes are reached, the data will be despooled and written
to tape.

Spooling data prevents interleaving data from several job and reduces or
eliminates tape drive stop and start commonly known as "shoe-shine".

We don't recommend using this option if you are writing to a disk file
using this option will probably just slow down the backup jobs.

NOTE: When this directive is set to yes, Spool Attributes is also
automatically set to yes.

\item [Spool Attributes = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Spool Attributes}
\index[dir]{Directive!Spool Attributes}
\index[dir]{slow}
\index[general]{slow}
\index[dir]{Backups!slow}
\index[general]{Backups!slow}
The default is set to {\bf no}, which means that the File attributes are
sent by the Storage daemon to the Director as they are stored on tape.
However, if you want to avoid the possibility that database updates will
slow down writing to the tape, you may want to set the value to {\bf
yes}, in which case the Storage daemon will buffer the File attributes
and Storage coordinates to a temporary file in the Working Directory,
then when writing the Job data to the tape is completed, the attributes
and storage coordinates will be sent to the Director.

NOTE: When Spool Data is set to yes, Spool Attributes is also
automatically set to yes.

\item [SpoolSize={\textless}bytes{\textgreater}] \hfill \\
\index[dir]{SpoolSize}
\index[dir]{Directive!SpoolSize}
where the bytes specify the maximum spool size for this job.
The default is take from Device Maximum Spool Size limit.


\item [Where = {\textless}directory{\textgreater}] \hfill \\
\index[dir]{Where}
\index[dir]{Directive!Where}
This directive applies only to a Restore job and specifies a prefix to
the directory name of all files being restored.  This permits files to
be restored in a different location from which they were saved.  If {\bf
Where} is not specified or is set to backslash ({\bf /}), the files will
be restored to their original location.  By default, we have set {\bf
Where} in the example configuration files to be {\bf
/tmp/bareos-restores}.  This is to prevent accidental overwriting of
your files.

\item [Add Prefix = {\textless}directory{\textgreater}] \hfill \\
\label{confaddprefix}
\index[dir]{AddPrefix}
\index[dir]{Directive!AddPrefix}
This directive applies only to a Restore job and specifies a prefix to the
directory name of all files being restored.  This will use \ilink{File
Relocation}{filerelocation} feature.

\item [Add Suffix = {\textless}extention{\textgreater}] \hfill \\
\index[dir]{AddSuffix}
\index[dir]{Directive!AddSuffix}
This directive applies only to a Restore job and specifies a suffix to all
files being restored.  This will use \ilink{File Relocation}{filerelocation}
feature.

Using \texttt{Add Suffix=.old}, \texttt{/etc/passwd} will be restored to
\texttt{/etc/passwsd.old}

\item [Strip Prefix = {\textless}directory{\textgreater}] \hfill \\
\index[dir]{StripPrefix}
\index[dir]{Directive!StripPrefix}
This directive applies only to a Restore job and specifies a prefix to remove
from the directory name of all files being restored.  This will use the
\ilink{File Relocation}{filerelocation} feature.

Using \texttt{Strip Prefix=/etc}, \texttt{/etc/passwd} will be restored to
\texttt{/passwd}

Under Windows, if you want to restore \texttt{c:/files} to \texttt{d:/files},
you can use :

\begin{verbatim}
 Strip Prefix = c:
 Add Prefix = d:
\end{verbatim}

\item [RegexWhere = {\textless}expressions{\textgreater}] \hfill \\
\index[dir]{RegexWhere}
\index[dir]{Directive!RegexWhere}
This directive applies only to a Restore job and specifies a regex filename
manipulation of all files being restored.  This will use \ilink{File
Relocation}{filerelocation} feature.

For more informations about how use this option, see
\ilink{this}{regexwhere}.

\item [Replace = {\textless}replace-option{\textgreater}] \hfill \\
\index[dir]{Replace}
\index[dir]{Directive!Replace}
This directive applies only to a Restore job and specifies what happens
when Bareos wants to restore a file or directory that already exists.
You have the following options for {\bf replace-option}:

\begin{description}

\item [always]
\index[dir]{always}
when the file to be restored already exists, it is deleted and then
replaced by the copy that was backed up.  This is the default value.

\item [ifnewer]
\index[dir]{ifnewer}
if the backed up file (on tape) is newer than the existing file, the
existing file is deleted and replaced by the back up.

\item [ifolder]
\index[dir]{ifolder}
if the backed up file (on tape) is older than the existing file, the
existing file is deleted and replaced by the back up.

\item [never]
\index[dir]{never}
if the backed up file already exists, Bareos skips  restoring this file.
\end{description}

\item [Prefix Links={\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Prefix Links}
\index[dir]{Directive!Prefix Links}
If a {\bf Where} path prefix is specified for a recovery job, apply it
to absolute links as well.  The default is {\bf No}.  When set to {\bf
Yes} then while restoring files to an alternate directory, any absolute
soft links will also be modified to point to the new alternate
directory.  Normally this is what is desired -- i.e.  everything is self
consistent.  However, if you wish to later move the files to their
original locations, all files linked with absolute names will be broken.

\item [Maximum Concurrent Jobs = {\textless}number{\textgreater}] \hfill \\
\index[dir]{Maximum Concurrent Jobs}
\index[dir]{Directive!Maximum Concurrent Jobs}
where {\textless}number{\textgreater} is the maximum number of Jobs from the current
Job resource that can run concurrently.  Note, this directive limits
only Jobs with the same name as the resource in which it appears.  Any
other restrictions on the maximum concurrent jobs such as in the
Director, Client, or Storage resources will also apply in addition to
the limit specified here.  The default is set to 1, but you may set it
to a larger number.  We strongly recommend that you read the WARNING
documented under \ilink{ Maximum Concurrent Jobs}{DirMaxConJobs} in the
Director's resource.

\item [Reschedule On Error = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Reschedule On Error}
\index[dir]{Directive!Reschedule On Error}
If this directive is enabled, and the job terminates in error, the job
will be rescheduled as determined by the {\bf Reschedule Interval} and
{\bf Reschedule Times} directives.  If you cancel the job, it will not
be rescheduled.  The default is {\bf no} (i.e.  the job will not be
rescheduled).

This specification can be useful for portables, laptops, or other
machines that are not always connected to the network or switched on.

\item [Reschedule Interval = {\textless}time-specification{\textgreater}] \hfill \\
\index[dir]{Reschedule Interval}
\index[dir]{Directive!Reschedule Interval}
If you have specified {\bf Reschedule On Error = yes} and the job
terminates in error, it will be rescheduled after the interval of time
specified by {\bf time-specification}.  See \ilink{the time
specification formats}{Time} in the Configure chapter for details of
time specifications.  If no interval is specified, the job will not be
rescheduled on error.

\item [Reschedule Times = {\textless}count{\textgreater}] \hfill \\
\index[dir]{Reschedule Times}
\index[dir]{Directive!Reschedule Times}
This directive specifies the maximum number of times to reschedule the
job.  If it is set to zero (the default) the job will be rescheduled an
indefinite number of times.

\item [Allow Duplicate Jobs = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[general]{Allow Duplicate Jobs}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=13cm]{\idir duplicate-real}
  \caption{Allow Duplicate Jobs usage}
  \label{fig:allowduplicatejobs}
\end{figure}
A duplicate job in the sense we use it here means a second or subsequent job
with the same name starts.  This happens most frequently when the first job
runs longer than expected because no tapes are available.

If this directive is enabled duplicate jobs will be run.  If
the directive is set to {\bf no} then only one job of a given name
may run at one time, and the action that Bareos takes to ensure only
one job runs is determined by the other directives (see below).

If {\bf Allow Duplicate Jobs} is set to {\bf no} and two jobs
are present and none of the three directives given below permit
cancelling a job, then the current job (the second one started)
will be cancelled.

The default is {\bf yes}.

% \item [Allow Higher Duplicates = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
% \index[general]{Allow Higher Duplicates}
% This directive was implemented in version 5.0.0, but does not work
% as expected. If used, it should always be set to no.  In later versions
% of Bareos the directive is disabled (disregarded).

\item [Cancel Lower Level Duplicates = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[general]{Cancel Lower Level Duplicates}
If \textbf{Allow Duplicates Jobs} is set to \textbf{no} and this
directive is set to \textbf{yes}, Bareos will choose between duplicated
jobs the one with the highest level.  For example, it will cancel a
previous Incremental to run a Full backup.  It works only for Backup
jobs.  The default is \texttt{no}. If the levels of the duplicated
jobs are the same, nothing is done and the other
Cancel XXX Duplicate directives will be examined.

\item [Cancel Queued Duplicates = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[general]{Cancel Queued Duplicates}
If {\bf Allow Duplicate Jobs} is set to {\bf no} and
if this directive is set to {\bf yes} any job that is
already queued to run but not yet running will be canceled.
The default is {\bf no}.

\item[Cancel Running Duplicates = {\textless}yes{\textbar}no{\textgreater}]
\index[general]{Cancel Running Duplicates}
If {\bf Allow Duplicate Jobs} is set to {\bf no} and
if this directive is set to {\bf yes} any job that is already running
will be canceled.  The default is {\bf no}.

%%\item[DuplicateJobProximity = {\textless}time-specification{\textgreater}]
%%\index[general]{Duplicate Job Proximity}
%%This directive permits to determine if two jobs are really duplicated.
%%If the first one is running for long time, this is probably not a good
%%idea to cancel it.

\item [Run = {\textless}job-name{\textgreater}]
\index[dir]{Run}
\index[dir]{Directive!Run}
\index[dir]{Clone a Job}
The Run directive (not to be confused with the Run option in a
Schedule) allows you to start other jobs or to clone jobs. By using the
cloning keywords (see below), you can backup
the same data (or almost the same data) to two or more drives
at the same time. The {\bf job-name} is normally the same name
as the current Job resource (thus creating a clone). However, it
may be any Job name, so one job may start other related jobs.

The part after the equal sign must be enclosed in double quotes,
and can contain any string or set of options (overrides) that you
can specify when entering the Run command from the console. For
example {\bf storage=DDS-4 ...}.  In addition, there are two special
keywords that permit you to clone the current job. They are {\bf level=\%l}
and {\bf since=\%s}. The \%l in the level keyword permits
entering the actual level of the current job and the \%s in the since
keyword permits putting the same time for comparison as used on the
current job.  Note, in the case of the since keyword, the \%s must be
enclosed in double quotes, and thus they must be preceded by a backslash
since they are already inside quotes. For example:

\begin{verbatim}
   run = "Nightly-backup level=%l since=\"%s\" storage=DDS-4"
\end{verbatim}

A cloned job will not start additional clones, so it is not
possible to recurse.

Please note that all cloned jobs, as specified in the Run directives are
submitted for running before the original job is run (while it is being
initialized). This means that any clone job will actually start before
the original job, and may even block the original job from starting
until the original job finishes unless you allow multiple simultaneous
jobs.  Even if you set a lower priority on the clone job, if no other
jobs are running, it will start before the original job.

If you are trying to prioritize jobs by using the clone feature (Run
directive), you will find it much easier to do using a RunScript
resource, or a RunBeforeJob directive.

\label{Priority}
\item [Priority = {\textless}number{\textgreater}]
\index[dir]{Priority}
\index[dir]{Directive!Priority}
This directive permits you to control the order in which your jobs will
be run by specifying a positive non-zero number. The higher the number,
the lower the job priority. Assuming you are not running concurrent jobs,
all queued jobs of priority 1 will run before queued jobs of priority 2
and so on, regardless of the original scheduling order.

The priority only affects waiting jobs that are queued to run, not jobs
that are already running.  If one or more jobs of priority 2 are already
running, and a new job is scheduled with priority 1, the currently
running priority 2 jobs must complete before the priority 1 job is
run, unless Allow Mixed Priority is set.

The default priority is 10.

If you want to run concurrent jobs you should
keep these points in mind:

\begin{itemize}
\item See \ilink{Running Concurrent Jobs}{ConcurrentJobs} on how to setup
concurrent jobs.

\item Bareos concurrently runs jobs of only one priority at a time.  It
will not simultaneously run a priority 1 and a priority 2 job.

\item If Bareos is running a priority 2 job and a new priority 1 job is
scheduled, it will wait until the running priority 2 job terminates even
if the Maximum Concurrent Jobs settings would otherwise allow two jobs
to run simultaneously.

\item Suppose that bareos is running a priority 2 job and a new priority 1
job is scheduled and queued waiting for the running priority 2 job to
terminate.  If you then start a second priority 2 job, the waiting
priority 1 job will prevent the new priority 2 job from running
concurrently with the running priority 2 job.  That is: as long as there
is a higher priority job waiting to run, no new lower priority jobs will
start even if the Maximum Concurrent Jobs settings would normally allow
them to run.  This ensures that higher priority jobs will be run as soon
as possible.
\end{itemize}

If you have several jobs of different priority, it may not best to start
them at exactly the same time, because Bareos must examine them one at a
time.  If by Bareos starts a lower priority job first, then it will run
before your high priority jobs.  If you experience this problem, you may
avoid it by starting any higher priority jobs a few seconds before lower
priority ones.  This insures that Bareos will examine the jobs in the
correct order, and that your priority scheme will be respected.

\label{AllowMixedPriority}
\item [Allow Mixed Priority = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Allow Mixed Priority}
When
set to {\bf yes} (default {\bf no}), this job may run even if lower
priority jobs are already running.  This means a high priority job
will not have to wait for other jobs to finish before starting.
The scheduler will only mix priorities when all running jobs have
this set to true.

Note that only higher priority jobs will start early.  Suppose the
director will allow two concurrent jobs, and that two jobs with
priority 10 are running, with two more in the queue.  If a job with
priority 5 is added to the queue, it will be run as soon as one of
the running jobs finishes.  However, new priority 10 jobs will not
be run until the priority 5 job has finished.
\end{description}

The following is an example of a valid Job resource definition:

\footnotesize
\begin{verbatim}
Job {
  Name = "Minou"
  Type = Backup
  Level = Incremental                 # default
  Client = Minou
  FileSet="Minou Full Set"
  Storage = DLTDrive
  Pool = Default
  Schedule = "MinouWeeklyCycle"
  Messages = Standard
}
\end{verbatim}
\normalsize

\section{JobDefs Resource}
\label{JobDefsResource}
\index[general]{JobDefs Resource}
\index[general]{Resource!JobDefs}

The JobDefs resource permits all the same directives that can appear in a Job
resource. However, a JobDefs resource does not create a Job, rather it can be
referenced within a Job to provide defaults for that Job. This permits you to
concisely define several nearly identical Jobs, each one referencing a JobDefs
resource which contains the defaults. Only the changes from the defaults need to
be mentioned in each Job.

\section{Schedule Resource}
\label{ScheduleResource}
\index[general]{Resource!Schedule}
\index[general]{Schedule Resource}

The Schedule resource provides a means of automatically scheduling a Job as
well as the ability to override the default Level, Pool, Storage and Messages
resources. If a Schedule resource is not referenced in a Job, the Job can only
be run manually. In general, you specify an action to be taken and when.

\begin{description}

\item [Schedule] \hfill \\
\index[dir]{Schedule}
\index[dir]{Directive!Schedule}
Start of the Schedule directives.  No {\bf Schedule} resource is
required, but you will need at least one if you want Jobs to be
automatically started.

\item [Name = {\textless}name{\textgreater}] \hfill \\
\index[dir]{Name}
\index[dir]{Directive!Name}
The name of the schedule being defined.  The Name directive is required.

\item [Run = {\textless}Job-overrides{\textgreater} {\textless}Date-time-specification{\textgreater}] \hfill \\
\index[dir]{Run}
\index[dir]{Directive!Run}
The Run directive defines when a Job is to be run, and what overrides if
any to apply.  You may specify multiple {\bf run} directives within a
{\bf Schedule} resource.  If you do, they will all be applied (i.e.
multiple schedules).  If you have two {\bf Run} directives that start at
the same time, two Jobs will start at the same time (well, within one
second of each other).

The {\bf Job-overrides} permit overriding the Level, the Storage, the
Messages, and the Pool specifications provided in the Job resource.  In
addition, the FullPool, the IncrementalPool, and the DifferentialPool
specifications permit overriding the Pool specification according to
what backup Job Level is in effect.

By the use of overrides, you may customize a particular Job.  For
example, you may specify a Messages override for your Incremental
backups that outputs messages to a log file, but for your weekly or
monthly Full backups, you may send the output by email by using a
different Messages override.

{\bf Job-overrides} are specified as: {\bf keyword=value} where the
keyword is Level, Storage, Messages, Pool, FullPool, DifferentialPool,
or IncrementalPool, and the {\bf value} is as defined on the respective
directive formats for the Job resource.  You may specify multiple {\bf
Job-overrides} on one {\bf Run} directive by separating them with one or
more spaces or by separating them with a trailing comma.  For example:

\begin{description}

\item [Level=Full]
\index[dir]{Level}
\index[dir]{Directive!Level}
is all files in the FileSet whether or not  they have changed.

\item [Level=Incremental]
\index[dir]{Level}
\index[dir]{Directive!Level}
is all files that have changed since  the last backup.

\item [Pool=Weekly]
\index[dir]{Pool}
\index[dir]{Directive!Pool}
specifies to use the Pool named {\bf Weekly}.

\item [Storage=DLT\_Drive]
\index[dir]{Storage}
\index[dir]{Directive!Storage}
specifies to use {\bf DLT\_Drive} for  the storage device.

\item [Messages=Verbose]
\index[dir]{Messages}
\index[dir]{Directive!Messages}
specifies to use the {\bf Verbose}  message resource for the Job.

\item [FullPool=Full]
\index[dir]{FullPool}
\index[dir]{Directive!FullPool}
specifies to use the Pool named {\bf Full}  if the job is a full backup, or
is upgraded from another type  to a full backup.

\item [DifferentialPool=Differential]
\index[dir]{DifferentialPool}
\index[dir]{Directive!DifferentialPool}
specifies to use the Pool named {\bf Differential} if the job is a
differential  backup.

\item [IncrementalPool=Incremental]
\index[dir]{IncrementalPool}
\index[dir]{Directive!IncrementalPool}
specifies to use the Pool  named {\bf Incremental} if the job is an
incremental  backup.

\item [Accurate=yes{\textbar}no]
\index[dir]{Accurate}
\index[dir]{Directive!Accurate}
tells Bareos to use or not the Accurate code for the specific job. It can
allow you to save memory and and CPU resources on the catalog server in some
cases.

\end{description}

{\bf Date-time-specification} determines when the  Job is to be run. The
specification is a repetition, and as  a default Bareos is set to run a job at
the beginning of the  hour of every hour of every day of every week of every
month  of every year. This is not normally what you want, so you  must specify
or limit when you want the job to run. Any  specification given is assumed to
be repetitive in nature and  will serve to override or limit the default
repetition. This  is done by specifying masks or times for the hour, day of the
month, day of the week, week of the month, week of the year,  and month when
you want the job to run. By specifying one or  more of the above, you can
define a schedule to repeat at  almost any frequency you want.

Basically, you must supply a {\bf month}, {\bf day}, {\bf hour}, and  {\bf
minute} the Job is to be run. Of these four items to be specified,  {\bf day}
is special in that you may either specify a day of the month  such as 1, 2,
... 31, or you may specify a day of the week such  as Monday, Tuesday, ...
Sunday. Finally, you may also specify a  week qualifier to restrict the
schedule to the first, second, third,  fourth, or fifth week of the month.

For example, if you specify only a day of the week, such as {\bf Tuesday}  the
Job will be run every hour of every Tuesday of every Month. That  is the {\bf
month} and {\bf hour} remain set to the defaults of  every month and all
hours.

Note, by default with no other specification, your job will run  at the
beginning of every hour. If you wish your job to run more than  once in any
given hour, you will need to specify multiple {\bf run}  specifications each
with a different minute.

The date/time to run the Job can be specified in the following way  in
pseudo-BNF:

\footnotesize
\begin{verbatim}
<void-keyword>    = on
<at-keyword>      = at
<week-keyword>    = 1st | 2nd | 3rd | 4th | 5th | first |
                    second | third | fourth | fifth
<wday-keyword>    = sun | mon | tue | wed | thu | fri | sat |
                    sunday | monday | tuesday | wednesday |
                    thursday | friday | saturday
<week-of-year-keyword> = w00 | w01 | ... w52 | w53
<month-keyword>   = jan | feb | mar | apr | may | jun | jul |
                    aug | sep | oct | nov | dec | january |
                    february | ... | december
<daily-keyword>   = daily
<weekly-keyword>  = weekly
<monthly-keyword> = monthly
<hourly-keyword>  = hourly
<digit>           = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 0
<number>          = <digit> | <digit><number>
<12hour>          = 0 | 1 | 2 | ... 12
<hour>            = 0 | 1 | 2 | ... 23
<minute>          = 0 | 1 | 2 | ... 59
<day>             = 1 | 2 | ... 31
<time>            = <hour>:<minute> |
                    <12hour>:<minute>am |
                    <12hour>:<minute>pm
<time-spec>       = <at-keyword> <time> |
                    <hourly-keyword>
<date-keyword>    = <void-keyword>  <weekly-keyword>
<day-range>       = <day>-<day>
<month-range>     = <month-keyword>-<month-keyword>
<wday-range>      = <wday-keyword>-<wday-keyword>
<range>           = <day-range> | <month-range> |
                          <wday-range>
<date>            = <date-keyword> | <day> | <range>
<date-spec>       = <date> | <date-spec>
<day-spec>        = <day> | <wday-keyword> |
                    <day> | <wday-range> |
                    <week-keyword> <wday-keyword> |
                    <week-keyword> <wday-range> |
                    <daily-keyword>
<month-spec>      = <month-keyword> | <month-range> |
                    <monthly-keyword>
<date-time-spec>  = <month-spec> <day-spec> <time-spec>
\end{verbatim}
\normalsize

\end{description}

Note, the Week of Year specification wnn follows the ISO standard definition
of the week of the year, where Week 1 is the week in which the first Thursday
of the year occurs, or alternatively, the week which contains the 4th of
January. Weeks are numbered w01 to w53. w00 for Bareos is the week that
precedes the first ISO week (i.e. has the first few days of the year if any
occur before Thursday). w00 is not defined by the ISO specification. A week
starts with Monday and ends with Sunday.

According to the NIST (US National Institute of Standards and Technology),
12am and 12pm are ambiguous and can be defined to anything.  However,
12:01am is the same as 00:01 and 12:01pm is the same as 12:01, so Bareos
defines 12am as 00:00 (midnight) and 12pm as 12:00 (noon).  You can avoid
this abiguity (confusion) by using 24 hour time specifications (i.e.  no
am/pm).

An example schedule resource that is named {\bf WeeklyCycle} and runs a job
with level full each Sunday at 2:05am and an incremental job Monday through
Saturday at 2:05am is:

\footnotesize
\begin{verbatim}
Schedule {
  Name = "WeeklyCycle"
  Run = Level=Full sun at 2:05
  Run = Level=Incremental mon-sat at 2:05
}
\end{verbatim}
\normalsize

An example of a possible monthly cycle is as follows:

\footnotesize
\begin{verbatim}
Schedule {
  Name = "MonthlyCycle"
  Run = Level=Full Pool=Monthly 1st sun at 2:05
  Run = Level=Differential 2nd-5th sun at 2:05
  Run = Level=Incremental Pool=Daily mon-sat at 2:05
}
\end{verbatim}
\normalsize

The first of every month:

\footnotesize
\begin{verbatim}
Schedule {
  Name = "First"
  Run = Level=Full on 1 at 2:05
  Run = Level=Incremental on 2-31 at 2:05
}
\end{verbatim}
\normalsize

Every 10 minutes:

\footnotesize
\begin{verbatim}
Schedule {
  Name = "TenMinutes"
  Run = Level=Full hourly at 0:05
  Run = Level=Full hourly at 0:15
  Run = Level=Full hourly at 0:25
  Run = Level=Full hourly at 0:35
  Run = Level=Full hourly at 0:45
  Run = Level=Full hourly at 0:55
}
\end{verbatim}
\normalsize

\subsection{Technical Notes on Schedules}
\index[general]{Schedules!Technical Notes on}
\index[general]{Technical Notes on Schedules}

Internally Bareos keeps a schedule as a bit mask. There are six masks and a
minute field to each schedule. The masks are hour, day of the month (mday),
month, day of the week (wday), week of the month (wom), and week of the year
(woy). The schedule is initialized to have the bits of each of these masks
set, which means that at the beginning of every hour, the job will run. When
you specify a month for the first time, the mask will be cleared and the bit
corresponding to your selected month will be selected. If you specify a second
month, the bit corresponding to it will also be added to the mask. Thus when
Bareos checks the masks to see if the bits are set corresponding to the
current time, your job will run only in the two months you have set. Likewise,
if you set a time (hour), the hour mask will be cleared, and the hour you
specify will be set in the bit mask and the minutes will be stored in the
minute field.

For any schedule you have defined, you can see how these bits are set by doing
a {\bf show schedules} command in the Console program. Please note that the
bit mask is zero based, and Sunday is the first day of the week (bit zero).

\input{dirdconf-fileset}

\section{Client Resource}
\label{ClientResource2}
\index[general]{Resource!Client}
\index[general]{Client Resource}

The Client resource defines the attributes of the Clients that are served by
this Director; that is the machines that are to be backed up. You will need
one Client resource definition for each machine to be backed up.

\begin{description}

\item [Client (or FileDaemon)]
\index[dir]{Client (or FileDaemon)}
\index[dir]{Directive!Client (or FileDaemon)}
Start of the Client directives.

\item [Name = {\textless}name{\textgreater}] \hfill \\
\index[dir]{Name}
\index[dir]{Directive!Name}
The client name which will be used in the  Job resource directive or in the
console run command.  This directive is required.

\item [Protocol = {\textless}protocolname{\textgreater}] \hfill \\
\index[dir]{Protocol}
\index[dir]{Directive!Protocol}
The backup protocol to use to run the Job. If not set it will default
to {\bf Native} currently the director understand the following protocols:
\begin{enumerate}
\item Native - The native Bareos protocol
\item NDMP - The NDMP protocol
\end{enumerate}

\item [Authtype = {\textless}Client-Authtype{\textgreater}] \hfill \\
\index[dir]{Authtype}
\index[dir]{Directive!Authtype}
Specifies the authentication type that must be supplied when connecting to
a backup protocol that uses a specific authentication type.

The following values are allowed:
\begin{enumerate}
\item None - Use no password
\item Clear - Use clear text password
\item MD5 - Use MD5 hashing
\end{enumerate}

\item [Address = {\textless}address{\textgreater}] \hfill \\
\index[dir]{Address}
\index[dir]{Directive!FD Address}
\index[dir]{File Daemon Address}
\index[dir]{Client Address}
Where the address is a host name, a fully qualified domain name, or a
network address in dotted quad notation for a Bareos File server daemon.
This directive is required.

\item [FD Port = {\textless}port-number{\textgreater}] \hfill \\
\index[dir]{FD Port}
\index[dir]{Directive!FD Port}
Where the port is a port  number at which the Bareos File server daemon can
be contacted.  The default is 9102. For NDMP backups set this to 10000.

\item [Catalog = {\textless}Catalog-resource-name{\textgreater}] \hfill \\
\index[dir]{Catalog}
\index[dir]{Directive!Catalog}
This specifies the  name of the catalog resource to be used for this Client.
If none is specified the first defined catalog is used.

\item [Username = {\textless}Client-Username{\textgreater}] \hfill \\
\index[dir]{Username}
\index[dir]{Directive!Username}
Specifies the username that must be supplied when authenticating.
Only used for the non Native protocols at the moment.

\item [Password = {\textless}password{\textgreater}] \hfill \\
\index[dir]{Password}
\index[dir]{Directive!Password}
This is the password to be  used when establishing a connection with the File
services, so  the Client configuration file on the machine to be backed up
must  have the same password defined for this Director. This directive is
required.  If you have either {\bf /dev/random}  {\bf bc} on your machine,
Bareos will generate a random  password during the configuration process,
otherwise it will  be left blank.

The password is plain text.  It is not generated through any special
process, but it is preferable for security reasons to make the text
random.

\item [Hard Quota = {\textless}amount{\textgreater}] \hfill \\
\index[dir]{Hard Quota}
\index[dir]{Directive!Hard Quota}
This is the maximal amount this client can backup before any backup Job
will be aborted.

\item [Soft Quota = {\textless}amount{\textgreater}] \hfill \\
\index[dir]{Soft Quota}
\index[dir]{Directive!Soft Quota}
This is the amount after which there will be a warning issued that a client
is over his softquota. A client can keep doing backups until it hits the
hardquota or when the Soft Quota Graceperiod is expired.

\item [Soft Quota Grace Period = {\textless}time interval{\textgreater}] \hfill \\
\index[dir]{Soft Quota Grace Period}
\index[dir]{Directive!Soft Quota Grace Period}
Time allowed for a client to be over its softquota before it will be enforced.

\item [Strict Quotas = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Strict Quota}
\index[dir]{Directive!Strict Quota}
The directive Strict Quotas determines, if after the Grace Time Period is over,
the Burst Limit is enforced (Strict Quotas = {\bf No}) or
the Soft Limit is enforced (Strict Quotas = {\bf Yes}).

\item [Quota Include Failed Jobs = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Quota Include Failed Jobs}
\index[dir]{Directive!Quota Include Failed Jobs}
When calculating the amount a client used take into consideration any failed Jobs.
Default {\bf Yes}.

\item [File Retention = {\textless}time-period-specification{\textgreater}] \hfill \\
\label{FileRetention}
\index[dir]{File Retention}
\index[dir]{Directive!File Retention}
The File Retention directive defines the length of time that  Bareos will
keep File records in the Catalog database after the End time of the
Job corresponding to the File records.
When this time period expires, and if
{\bf AutoPrune} is set to  {\bf yes} Bareos will prune (remove) File records
that  are older than the specified File Retention period. Note, this  affects
only records in the catalog database. It does not  affect your archive
backups.

File records  may actually be retained for a shorter period than you specify
on  this directive if you specify either a shorter {\bf Job Retention}  or a
shorter {\bf Volume Retention} period. The shortest  retention period of the
three takes precedence.  The time may be expressed in seconds, minutes,
hours, days, weeks, months, quarters, or years. See the
\ilink{ Configuration chapter}{Time} of this  manual for
additional details of time specification.

The  default is 60 days.

\item [Job Retention = {\textless}time-period-specification{\textgreater}] \hfill \\
\label{JobRetention}
\index[dir]{Job Retention}
\index[dir]{Directive!Job Retention}
The Job Retention directive defines the length of time that  Bareos will keep
Job records in the Catalog database after the Job End time.  When
this time period expires, and if {\bf AutoPrune} is set to {\bf yes}
Bareos will prune (remove) Job records that are older than the specified
File Retention period.  As with the other retention periods, this
affects only records in the catalog and not data in your archive backup.

If a Job record is selected for pruning, all associated File and JobMedia
records will also be pruned regardless of the File Retention period set.
As a consequence, you normally will set the File retention period to be
less than the Job retention period.  The Job retention period can actually
be less than the value you specify here if you set the {\bf Volume
Retention} directive in the Pool resource to a smaller duration.  This is
because the Job retention period and the Volume retention period are
independently applied, so the smaller of the two takes precedence.

The Job retention period is specified as seconds,  minutes, hours, days,
weeks, months,  quarters, or years.  See the
\ilink{ Configuration chapter}{Time} of this manual for
additional details of  time specification.

The default is 180 days.

\label{AutoPrune}
\item [AutoPrune = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{AutoPrune}
\index[dir]{Directive!AutoPrune}
If AutoPrune is set to  {\bf yes} (default), Bareos (version 1.20 or greater)
will  automatically apply the File retention period and the Job  retention
period for the Client at the end of the Job.  If you set {\bf AutoPrune = no},
pruning will not be done,  and your Catalog will grow in size each time you
run a Job.  Pruning affects only information in the catalog and not data
stored in the backup archives (on Volumes).

\item [Maximum Concurrent Jobs = {\textless}number{\textgreater}] \hfill \\
\index[dir]{Maximum Concurrent Jobs}
\index[dir]{Directive!Maximum Concurrent Jobs}
where {\textless}number{\textgreater}  is the maximum number of Jobs with the current Client
that  can run concurrently. Note, this directive limits only Jobs  for Clients
with the same name as the resource in which it appears. Any  other
restrictions on the maximum concurrent jobs such as in  the Director, Job, or
Storage resources will also apply in addition to  any limit specified here.
The  default is set to 1, but you may set it to a larger number.

\item [Maximum Bandwidth Per Job = {\textless}speed{\textgreater}] \hfill \\
\index[dir]{Maximum Bandwidth Per Job}
\index[dir]{Directive!Maximum Bandwidth Per Job}
The speed parameter specifies the maximum allowed bandwidth that a job may use
when started for this Client. The speed parameter should be specified in
k/s, Kb/s, m/s or Mb/s.

\item [NDMP Loglevel = {\textless}level{\textgreater}] \hfill \\
\index[sd]{NDMP Loglevel}
\index[sd]{Directive!NDMP Loglevel}
This directive sets the loglevel for the NDMP protocol library.

\item [NDMP Blocksize = {\textless}blocksize{\textgreater}] \hfill \\
\index[sd]{NDMP Blocksize}
\index[sd]{Directive!NDMP Blocksize}
This directive sets the default NDMP blocksize for this client.

\item [Priority = {\textless}number{\textgreater}] \hfill \\
\index[dir]{Priority}
\index[dir]{Directive!Priority}
The number specifies the  priority of this client relative to other clients
that the  Director is processing simultaneously. The priority can range  from
1 to 1000. The clients are ordered such that the smaller  number priorities
are performed first (not currently  implemented).

\end{description}

The following is an example of a valid Client resource definition:

\footnotesize
\begin{verbatim}
Client {
  Name = minimatou
  FDAddress = minimatou.example.com
  Password = "secret"
}
\end{verbatim}
\normalsize

\section{Storage Resource}
\label{StorageResource2}
\index[general]{Resource!Storage}
\index[general]{Storage Resource}

The Storage resource defines which Storage daemons are available for use by
the Director.

\begin{description}

\item [Storage]
\index[dir]{Storage}
\index[dir]{Directive!Storage}
Start of the Storage resources. At least one  storage resource must be
specified.

\item [Name = {\textless}name{\textgreater}] \hfill \\
\index[dir]{Name}
\index[dir]{Directive!Name}
The name of the storage resource. This  name appears on the Storage directive
specified in the Job resource and is required.

\item [Address = {\textless}address{\textgreater}] \hfill \\
\index[dir]{Address}
\index[dir]{Directive!SD Address}
\index[dir]{Storage daemon Address}
Where the address is a host name,  a {\bf fully qualified domain name}, or an
{\bf IP address}. Please note  that the {\textless}address{\textgreater} as specified here
will be transmitted to  the File daemon who will then use it to contact the
Storage daemon. Hence,  it is {\bf not}, a good idea to use {\bf localhost} as
the  name but rather a fully qualified machine name or an IP address.  This
directive is required.

\item [SD Port = {\textless}port{\textgreater}] \hfill \\
\index[dir]{SD Port}
\index[dir]{Directive!SD Port}
Where port is the port to use to  contact the storage daemon for information
and to start jobs.  This same port number must appear in the Storage resource
of the  Storage daemon's configuration file. The default is 9103.

\item [Password = {\textless}password{\textgreater}] \hfill \\
\index[dir]{Password}
\index[dir]{Directive!Password}
This is the password to be used  when establishing a connection with the
Storage services. This  same password also must appear in the Director
resource of the Storage  daemon's configuration file. This directive is
required.  If you have either {\bf /dev/random}  {\bf bc} on your machine,
Bareos will generate a random  password during the configuration process,
otherwise it will  be left blank.

The password is plain text.  It is not generated through any special
process, but it is preferable for security reasons to use random text.

\item [Device = {\textless}device-name{\textgreater}] \hfill \\
\index[dir]{Device}
\index[dir]{Directive!Device}
This directive specifies the Storage daemon's name of the device
resource to be used for the storage.  If you are using an Autochanger,
the name specified here should be the name of the Storage daemon's
Autochanger resource rather than the name of an individual device.  This
name is not the physical device name, but the logical device name as
defined on the {\bf Name} directive contained in the {\bf Device} or the
{\bf Autochanger} resource definition of the {\bf Storage daemon}
configuration file.  You can specify any name you would like (even the
device name if you prefer) up to a maximum of 127 characters in length.
The physical device name associated with this device is specified in the
{\bf Storage daemon} configuration file (as {\bf Archive Device}).
Please take care not to define two different Storage resource directives
in the Director that point to the same Device in the Storage daemon.
Doing so may cause the Storage daemon to block (or hang) attempting to
open the same device that is already open.  This directive is required.

\label{MediaType}
\item [Media Type = {\textless}MediaType{\textgreater}] \hfill \\
\index[dir]{Media Type}
\index[dir]{Directive!Media Type}
This directive specifies the Media Type to be used to store the data.
This is an arbitrary string of characters up to 127 maximum that you
define.  It can be anything you want.  However, it is best to make it
descriptive of the storage media (e.g.  File, DAT, "HP DLT8000", 8mm,
...).  In addition, it is essential that you make the {\bf Media Type}
specification unique for each storage media type.  If you have two DDS-4
drives that have incompatible formats, or if you have a DDS-4 drive and
a DDS-4 autochanger, you almost certainly should specify different {\bf
Media Types}.  During a restore, assuming a {\bf DDS-4} Media Type is
associated with the Job, Bareos can decide to use any Storage daemon
that supports Media Type {\bf DDS-4} and on any drive that supports it.

If you are writing to disk Volumes, you must make doubly sure that each
Device resource defined in the Storage daemon (and hence in the
Director's conf file) has a unique media type.  Otherwise for Bareos
versions 1.38 and older, your restores may not work because Bareos
will assume that you can mount any Media Type with the same name on
any Device associated with that Media Type. This is possible with
tape drives, but with disk drives, unless you are very clever you
cannot mount a Volume in any directory -- this can be done by creating
an appropriate soft link.

Currently Bareos permits only a single Media Type per Storage
and Device definition. Consequently, if
you have a drive that supports more than one Media Type, you can
give a unique string to Volumes with different intrinsic Media
Type (Media Type = DDS-3-4 for DDS-3 and DDS-4 types), but then
those volumes will only be mounted on drives indicated with the
dual type (DDS-3-4).

If you want to tie Bareos to using a single Storage daemon or drive, you
must specify a unique Media Type for that drive.  This is an important
point that should be carefully understood.  Note, this applies equally
to Disk Volumes.  If you define more than one disk Device resource in
your Storage daemon's conf file, the Volumes on those two devices are in
fact incompatible because one can not be mounted on the other device
since they are found in different directories.  For this reason, you
probably should use two different Media Types for your two disk Devices
(even though you might think of them as both being File types).  You can
find more on this subject in the \ilink{Basic Volume
Management}{DiskChapter} chapter of this manual.

The {\bf MediaType} specified in the Director's Storage resource, {\bf
must} correspond to the {\bf Media Type} specified in the {\bf Device}
resource of the {\bf Storage daemon} configuration file.  This directive
is required, and it is used by the Director and the Storage daemon to
ensure that a Volume automatically selected from the Pool corresponds to
the physical device.  If a Storage daemon handles multiple devices (e.g.
will write to various file Volumes on different partitions), this
directive allows you to specify exactly which device.

As mentioned above, the value specified in the Director's Storage
resource must agree with the value specified in the Device resource in
the {\bf Storage daemon's} configuration file.  It is also an additional
check so that you don't try to write data for a DLT onto an 8mm device.

\label{Autochanger1}
\item [Autochanger = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Autochanger}
\index[dir]{Directive!Autochanger}
If you specify {\bf yes} for this command (the default is {\bf no}),
when you use the {\bf label} command or the {\bf add} command to create
a new Volume, {\bf Bareos} will also request the Autochanger Slot
number.  This simplifies creating database entries for Volumes in an
autochanger.  If you forget to specify the Slot, the autochanger will
not be used.  However, you may modify the Slot associated with a Volume
at any time by using the {\bf update volume} or {\bf update slots}
command in the console program.  When {\bf autochanger} is enabled, the
algorithm used by Bareos to search for available volumes will be
modified to consider only Volumes that are known to be in the
autochanger's magazine.  If no {\bf in changer} volume is found, Bareos
will attempt recycling, pruning, ..., and if still no volume is found,
Bareos will search for any volume whether or not in the magazine.  By
privileging in changer volumes, this procedure minimizes operator
intervention.  The default is {\bf no}.

For the autochanger to be used, you must also specify {\bf Autochanger =
yes} in the \ilink{Device Resource}{Autochanger} in the Storage daemon's
configuration file as well as other important Storage daemon
configuration information.  Please consult the \ilink{Using
Autochangers}{AutochangersChapter} manual of this chapter for the
details of using autochangers.

\item [Maximum Concurrent Jobs = {\textless}number{\textgreater}] \hfill \\
\index[dir]{Maximum Concurrent Jobs}
\index[dir]{Directive!Maximum Concurrent Jobs}
where {\textless}number{\textgreater}  is the maximum number of Jobs with the current
Storage resource that can run concurrently.  Note, this directive limits
only Jobs for Jobs using this Storage daemon.  Any other restrictions on
the maximum concurrent jobs such as in the Director, Job, or Client
resources will also apply in addition to any limit specified here.  The
default is set to 1, but you may set it to a larger number.  However, if
you set the Storage daemon's number of concurrent jobs greater than one,
we recommend that you read the waring documented under \ilink{Maximum
Concurrent Jobs}{DirMaxConJobs} in the Director's resource or simply
turn data spooling on as documented in the \ilink{Data
Spooling}{SpoolingChapter} chapter of this manual.

\item [Maximum Concurrent Read Jobs = {\textless}number{\textgreater}] \hfill \\
\index[dir]{Maximum Concurrent Read Jobs}
\index[dir]{Directive!Maximum Concurrent Read Jobs}
where {\textless}number{\textgreater}  is the maximum number of Jobs with the current
Storage resource that can read concurrently.

\item [AllowCompression = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\label{AllowCompression}
\index[dir]{AllowCompression}
\index[dir]{Directive!AllowCompression}

This directive is optional, and if you specify {\bf No} (the default is {\bf
  Yes}), it will cause backups jobs running on this storage resource to run
without client File Daemon compression.  This effectively overrides
compression options in FileSets used by jobs which use this storage
resource.

\item [Heartbeat Interval = {\textless}time-interval{\textgreater}] \hfill \\
\index[dir]{Heartbeat Interval}
\index[dir]{Directive!Heartbeat}
This directive is optional and if specified will cause the Director to
set a keepalive interval (heartbeat) in seconds on each of the sockets
it opens for the Storage resource.  This value will override any
specified at the Director level.  It is implemented only on systems
(Linux, ...) that provide the {\bf setsockopt} TCP\_KEEPIDLE function.
The default value is zero, which means no change is made to the socket.

\item [Paired Storage = {\textless}Storage-resource-name{\textgreater}] \hfill \\
\index[dir]{Paired Storage}
\index[dir]{Directive!Paired Storage}
For NDMP backups this points to the definition of the Native Storage
that is accesses via the NDMP protocol. For now we only support NDMP
backups and restores to access Native Storage Daemons via the NDMP
protocol. In the future we might allow to use Native NDMP storage which
is not bound to a Bareos Storage Daemon.
\end{description}

The following is an example of a valid Storage resource definition:

\footnotesize
\begin{verbatim}
# Definition of tape storage device
Storage {
  Name = DLTDrive
  Address = lpmatou
  Password = storage_password # password for Storage daemon
  Device = "HP DLT 80"    # same as Device in Storage daemon
  Media Type = DLT8000    # same as MediaType in Storage daemon
}
\end{verbatim}
\normalsize

\section{Pool Resource}
\label{PoolResource}
\index[general]{Resource!Pool}
\index[general]{Pool Resource}

The Pool resource defines the set of storage Volumes (tapes or files) to be
used by Bareos to write the data. By configuring different Pools, you can
determine which set of Volumes (media) receives the backup data. This permits,
for example, to store all full backup data on one set of Volumes and all
incremental backups on another set of Volumes. Alternatively, you could assign
a different set of Volumes to each machine that you backup. This is most
easily done by defining multiple Pools.

Another important aspect of a Pool is that it contains the default attributes
(Maximum Jobs, Retention Period, Recycle flag, ...) that will be given to a
Volume when it is created. This avoids the need for you to answer a large
number of questions when labeling a new Volume. Each of these attributes can
later be changed on a Volume by Volume basis using the {\bf update} command in
the console program. Note that you must explicitly specify which Pool Bareos
is to use with each Job. Bareos will not automatically search for the correct
Pool.

Most often in Bareos installations all backups for all machines (Clients) go
to a single set of Volumes. In this case, you will probably only use the {\bf
Default} Pool. If your backup strategy calls for you to mount a different tape
each day, you will probably want to define a separate Pool for each day. For
more information on this subject, please see the
\ilink{Backup Strategies}{StrategiesChapter} chapter of this
manual.

To use a Pool, there are three distinct steps. First the Pool must be defined
in the Director's configuration file. Then the Pool must be written to the
Catalog database. This is done automatically by the Director each time that it
starts, or alternatively can be done using the {\bf create} command in the
console program. Finally, if you change the Pool definition in the Director's
configuration file and restart Bareos, the pool will be updated alternatively
you can use the {\bf update pool} console command to refresh the database
image. It is this database image rather than the Director's resource image
that is used for the default Volume attributes. Note, for the pool to be
automatically created or updated, it must be explicitly referenced by a Job
resource.

Next the physical media must be labeled. The labeling can either be done with
the {\bf label} command in the {\bf console} program or using the {\bf btape}
program. The preferred method is to use the {\bf label} command in the {\bf
console} program.

Finally, you must add Volume names (and their attributes) to the Pool. For
Volumes to be used by Bareos they must be of the same {\bf Media Type} as the
archive device specified for the job (i.e. if you are going to back up to a
DLT device, the Pool must have DLT volumes defined since 8mm volumes cannot be
mounted on a DLT drive). The {\bf Media Type} has particular importance if you
are backing up to files. When running a Job, you must explicitly specify which
Pool to use. Bareos will then automatically select the next Volume to use from
the Pool, but it will ensure that the {\bf Media Type} of any Volume selected
from the Pool is identical to that required by the Storage resource you have
specified for the Job.

If you use the {\bf label} command in the console program to label the
Volumes, they will automatically be added to the Pool, so this last step is
not normally required.

It is also possible to add Volumes to the database without explicitly labeling
the physical volume. This is done with the {\bf add} console command.

As previously mentioned, each time Bareos starts, it scans all the Pools
associated with each Catalog, and if the database record does not already
exist, it will be created from the Pool Resource definition. {\bf Bareos}
probably should do an {\bf update pool} if you change the Pool definition, but
currently, you must do this manually using the {\bf update pool} command in
the Console program.

The Pool Resource defined in the Director's configuration file
(bareos-dir.conf) may contain the following directives:

\begin{description}
\item [Pool]
\index[dir]{Pool}
\index[dir]{Directive!Pool}
Start of the Pool resource.  There must be at least one Pool resource
defined.

\item [Name = {\textless}name{\textgreater}] \hfill \\
\index[dir]{Name}
\index[dir]{Directive!Name}
The name of the pool.  For most applications, you will use the default
pool name {\bf Default}.  This directive is required.

\label{MaxVolumes}
\item [Maximum Volumes = {\textless}number{\textgreater}] \hfill \\
\index[dir]{Maximum Volumes}
\index[dir]{Directive!Maximum Volumes}
This directive specifies the maximum number of volumes (tapes or files)
contained in the pool.  This directive is optional, if omitted or set to
zero, any number of volumes will be permitted.  In general, this
directive is useful for Autochangers where there is a fixed number of
Volumes, or for File storage where you wish to ensure that the backups
made to disk files do not become too numerous or consume too much space.

\item [Pool Type = {\textless}type{\textgreater}] \hfill \\
\index[dir]{Pool Type}
\index[dir]{Directive!Pool Type}
This directive defines the pool type, which corresponds to the type of
Job being run.  It is required and may be one of the following:

\begin{description}
  \item [Backup]
  \item [*Archive]
  \item [*Cloned]
  \item [*Migration]
  \item [*Copy]
  \item [*Save]
\end{description}
Note, only Backup is currently implemented.

\item [Storage = {\textless}storage-resource-name{\textgreater}] \hfill \\
\index[dir]{Storage}
\index[dir]{Directive!Storage}
The Storage directive defines the name of the storage services where you
want to backup the FileSet data.  For additional details, see the
\ilink{Storage Resource Chapter}{StorageResource2} of this manual.
The Storage resource may also be specified in the Job resource,
but the value, if any, in the Pool resource overrides any value
in the Job. This Storage resource definition is not required by either
the Job resource or in the Pool, but it must be specified in
one or the other.  If not configuration error will result.

\item [Use Volume Once = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Use Volume Once}
\index[dir]{Directive!Use Volume Once}
This directive if set to {\bf yes} specifies that each volume is to be
used only once.  This is most useful when the Media is a file and you
want a new file for each backup that is done.  The default is {\bf no}
(i.e.  use volume any number of times).  This directive will most likely
be phased out (deprecated), so you are recommended to use {\bf Maximum
Volume Jobs = 1} instead.

The value defined by this directive in the bareos-dir.conf file is the
default value used when a Volume is created.  Once the volume is
created, changing the value in the bareos-dir.conf file will not change
what is stored for the Volume.  To change the value for an existing
Volume you must use the {\bf update} command in the Console.

Please see the notes below under {\bf Maximum Volume Jobs} concerning
using this directive with multiple simultaneous jobs.

\item [Maximum Volume Jobs = {\textless}positive-integer{\textgreater}] \hfill \\
\index[dir]{Maximum Volume Jobs}
\index[dir]{Directive!Maximum Volume Jobs}
This directive specifies the maximum number of Jobs that can be written
to the Volume.  If you specify zero (the default), there is no limit.
Otherwise, when the number of Jobs backed up to the Volume equals {\bf
positive-integer} the Volume will be marked {\bf Used}.  When the Volume
is marked {\bf Used} it can no longer be used for appending Jobs, much
like the {\bf Full} status but it can be recycled if recycling is
enabled, and thus used again.  By setting {\bf MaximumVolumeJobs} to
one, you get the same effect as setting {\bf UseVolumeOnce = yes}.

The value defined by this directive in the  bareos-dir.conf
file is the default value used when a Volume  is created. Once the volume is
created, changing the value  in the bareos-dir.conf file will not change what
is stored  for the Volume. To change the value for an existing Volume  you
must use the {\bf update} command in the Console.

If you are running multiple simultaneous jobs, this directive may not
work correctly because when a drive is reserved for a job, this
directive is not taken into account, so multiple jobs may try to
start writing to the Volume. At some point, when the Media record is
updated, multiple simultaneous jobs may fail since the Volume can no
longer be written.

\item [Maximum Volume Files = {\textless}positive-integer{\textgreater}] \hfill \\
\index[dir]{Maximum Volume Files}
\index[dir]{Directive!Maximum Volume Files}
This directive specifies the maximum number of files that can be written
to the Volume.  If you specify zero (the default), there is no limit.
Otherwise, when the number of files written to the Volume equals {\bf
positive-integer} the Volume will be marked {\bf Used}.  When the Volume
is marked {\bf Used} it can no longer be used for appending Jobs, much
like the {\bf Full} status but it can be recycled if recycling is
enabled and thus used again.  This value is checked and the {\bf Used}
status is set only at the end of a job that writes to the particular
volume.

The value defined by this directive in the bareos-dir.conf file is the
default value used when a Volume is created.  Once the volume is
created, changing the value in the bareos-dir.conf file will not change
what is stored for the Volume.  To change the value for an existing
Volume you must use the {\bf update} command in the Console.

\item [Maximum Volume Bytes = {\textless}size{\textgreater}] \hfill \\
\index[dir]{Maximum Volume Bytes}
\index[dir]{Directive!Maximum Volume Bytes}
This directive specifies the maximum number of bytes that can be written
to the Volume.  If you specify zero (the default), there is no limit
except the physical size of the Volume.  Otherwise, when the number of
bytes written to the Volume equals {\bf size} the Volume will be marked
{\bf Used}.  When the Volume is marked {\bf Used} it can no longer be
used for appending Jobs, much like the {\bf Full} status but it can be
recycled if recycling is enabled, and thus the Volume can be re-used
after recycling.  This value is checked and the {\bf Used} status set
while the job is writing to the particular volume.

This directive is particularly useful for restricting the size
of disk volumes, and will work correctly even in the case of
multiple simultaneous jobs writing to the volume.

The value defined by this directive in the bareos-dir.conf file is the
default value used when a Volume is created.  Once the volume is
created, changing the value in the bareos-dir.conf file will not change
what is stored for the Volume.  To change the value for an existing
Volume you must use the {\bf update} command in the Console.

\xdirective{dir}{Volume Use Duration}{time-period-specification}{}{0 (no limit)}{}
{The Volume Use Duration directive defines the time period that the
Volume can be written beginning from the time of first data write to the
Volume.  If the time-period specified is zero (the default), the Volume
can be written indefinitely.  Otherwise, the next time a job
runs that wants to access this Volume, and the time period from the
first write to the volume (the first Job written) exceeds the
time-period-specification, the Volume will be marked {\bf Used}, which
means that no more Jobs can be appended to the Volume, but it may be
recycled if recycling is enabled.
% Using the command {\bf
% status dir} applies algorithms similar to running jobs, so
% during such a command, the Volume status may also be changed.
Once the Volume is
recycled, it will be available for use again.

You might use this directive, for example, if you have a Volume used for
Incremental backups, and Volumes used for Weekly Full backups.  Once the
Full backup is done, you will want to use a different Incremental
Volume.  This can be accomplished by setting the Volume Use Duration for
the Incremental Volume to six days.  I.e.  it will be used for the 6
days following a Full save, then a different Incremental volume will be
used.  Be careful about setting the duration to short periods such as 23
hours, or you might experience problems of Bareos waiting for a tape
over the weekend only to complete the backups Monday morning when an
operator mounts a new tape.

% The use duration is checked and the {\bf Used} status is set only at the
% end of a job that writes to the particular volume, which means that even
% though the use duration may have expired, the catalog entry will not be
% updated until the next job that uses this volume is run. This
% directive is not intended to be used to limit volume sizes
% and will not work correctly (i.e. will fail jobs) if the use
% duration expires while multiple simultaneous jobs are writing
% to the volume.

Please note that the value defined by this directive in the  bareos-dir.conf
file is the default value used when a Volume  is created. Once the volume is
created, changing the value  in the bareos-dir.conf file will not change what
is stored  for the Volume. To change the value for an existing Volume  you
must use the
\ilink{\bf update volume}{UpdateCommand} command in the Console.
}

\item [Catalog Files = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Catalog Files}
\index[dir]{Directive!Catalog Files}
This directive defines whether or not you want the names of the files
that were saved to be put into the catalog.  The default is {\bf yes}.
The advantage of specifying {\bf Catalog Files = No} is that you will
have a significantly smaller Catalog database.  The disadvantage is that
you will not be able to produce a Catalog listing of the files backed up
for each Job (this is often called Browsing).  Also, without the File
entries in the catalog, you will not be able to use the Console {\bf
restore} command nor any other command that references File entries.

\item [Catalog = {\textless}Catalog-resource-name{\textgreater}] \hfill \\
\index[dir]{Catalog}
\index[dir]{Directive!Catalog}
This specifies the name of the catalog resource to be used for this Pool.
When a catalog is defined in a Pool it will override the definition in
the client (and the Catalog definition in a Job since 13.4.0). e.g.
this catalog setting takes precedence over any other definition.

\label{PoolAutoPrune}
\item [AutoPrune = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{AutoPrune}
\index[dir]{Directive!AutoPrune}
If AutoPrune is set to {\bf yes} (default), Bareos (version 1.20 or
greater) will automatically apply the Volume Retention period when new
Volume is needed and no appendable Volumes exist in the Pool.  Volume
pruning causes expired Jobs (older than the {\bf Volume Retention}
period) to be deleted from the Catalog and permits possible recycling of
the Volume.

\label{VolRetention}
\item [Volume Retention = {\textless}time-period-specification{\textgreater}] \hfill \\
\index[dir]{Volume Retention}
\index[dir]{Directive!Volume Retention}
The Volume Retention directive defines the length of time that {\bf
Bareos} will keep records associated with the Volume in
the Catalog database after the End time of each Job written to the
Volume.  When this time period expires, and if {\bf AutoPrune} is set to
{\bf yes} Bareos may prune (remove) Job records that are older than the
specified Volume Retention period if it is necessary to free up a
Volume.  Recycling will not occur until it is absolutely necessary to
free up a volume (i.e. no other writable volume exists).
All File records associated with pruned Jobs are also
pruned.  The time may be specified as seconds, minutes, hours, days,
weeks, months, quarters, or years.  The {\bf Volume Retention} is
applied independently of the {\bf Job Retention} and the {\bf File
Retention} periods defined in the Client resource.  This means that all
the retentions periods are applied in turn and that the shorter period
is the one that effectively takes precedence.  Note, that when the {\bf
Volume Retention} period has been reached, and it is necessary to obtain
a new volume, Bareos will prune both the Job and the File records.  This
pruning could also occur during a {\bf status dir} command because it
uses similar algorithms for finding the next available Volume.

It is important to know that when the Volume Retention period expires,
Bareos does not automatically recycle a Volume. It attempts to keep the
Volume data intact as long as possible before over writing the Volume.

By defining multiple Pools with different Volume Retention periods, you
may effectively have a set of tapes that is recycled weekly, another
Pool of tapes that is recycled monthly and so on.  However, one must
keep in mind that if your {\bf Volume Retention} period is too short, it
may prune the last valid Full backup, and hence until the next Full
backup is done, you will not have a complete backup of your system, and
in addition, the next Incremental or Differential backup will be
promoted to a Full backup.  As a consequence, the minimum {\bf Volume
Retention} period should be at twice the interval of your Full backups.
This means that if you do a Full backup once a month, the minimum Volume
retention period should be two months.

The default Volume retention period is 365 days, and either the default
or the value defined by this directive in the bareos-dir.conf file is
the default value used when a Volume is created.  Once the volume is
created, changing the value in the bareos-dir.conf file will not change
what is stored for the Volume.  To change the value for an existing
Volume you must use the {\bf update} command in the Console.

\item [Action On Purge = {\textless}Truncate{\textgreater}] \hfill \\
\index[dir]{actiononpurge}

This directive \textbf{ActionOnPurge=Truncate} instructs Bareos to truncate the
volume when it is purged with the \texttt{purge volume action=truncate}
command. It is useful to prevent disk based volumes from consuming too much
space.

\begin{verbatim}
Pool {
  Name = Default
  Action On Purge = Truncate
  ...
}
\end{verbatim}

You can schedule the truncate operation at the end of your CatalogBackup job
like in this example:

\begin{verbatim}
Job {
 Name = CatalogBackup
 ...
 RunScript {
   RunsWhen=After
   RunsOnClient=No
   Console = "purge volume action=all allpools storage=File"
 }
}
\end{verbatim}

\item [NextPool = {\textless}pool-resource-name{\textgreater}] \hfill \\
\index[dir]{NextPool}
\index[dir]{Directive!NextPool}
This directive specifies the Next pool a Migration or Copy Job
and a Virtual Backup Job will write their data too.

\label{PoolScratchPool}
\item [ScratchPool = {\textless}pool-resource-name{\textgreater}] \hfill \\
\index[dir]{ScratchPool}
\index[dir]{Directive!ScratchPool}
This directive permits to specify a dedicate \textsl{Scratch} for the
current pool. This pool will replace the special pool named \textsl{Scrach}
for volume selection. For more information about \textsl{Scratch} see
\ilink{Scratch Pool}{TheScratchPool} section of this manual. This is useful
when using multiple storage sharing the same mediatype or when you want to
dedicate volumes to a particular set of pool.

\label{PoolRecyclePool}
\item [RecyclePool = {\textless}pool-resource-name{\textgreater}] \hfill \\
\index[dir]{RecyclePool}
\index[dir]{Directive!RecyclePool}
This directive defines to which pool
the Volume will be placed (moved) when it is recycled. Without
this directive, a Volume will remain in the same pool when it is
recycled. With this directive, it can be moved automatically to any
existing pool during a recycle. This directive is probably most
useful when defined in the Scratch pool, so that volumes will
be recycled back into the Scratch pool. For more on the see the
\ilink{Scratch Pool}{TheScratchPool} section of this manual.

Although this directive is called RecyclePool, the Volume in
question is actually moved from its current pool to the one
you specify on this directive when Bareos prunes the Volume and
discovers that there are no records left in the catalog and hence
marks it as {\bf Purged}.

\label{PoolRecycle}
\item [Recycle = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Recycle}
\index[dir]{Directive!Recycle}
This directive specifies whether or not Purged Volumes may be recycled.
If it is set to {\bf yes} (default) and Bareos needs a volume but finds
none that are appendable, it will search for and recycle (reuse) Purged
Volumes (i.e.  volumes with all the Jobs and Files expired and thus
deleted from the Catalog).  If the Volume is recycled, all previous data
written to that Volume will be overwritten. If Recycle is set to {\bf
no}, the Volume will not be recycled, and hence, the data will remain
valid.  If you want to reuse (re-write) the Volume, and the recycle flag
is no (0 in the catalog), you may manually set the recycle flag (update
command) for a Volume to be reused.

Please note that the value defined by this directive in the
bareos-dir.conf file is the default value used when a Volume is created.
Once the volume is created, changing the value in the bareos-dir.conf
file will not change what is stored for the Volume.  To change the value
for an existing Volume you must use the {\bf update} command in the
Console.

When all Job and File records have been pruned or purged from the
catalog for a particular Volume, if that Volume is marked as
Append, Full, Used, or Error, it will then be marked as Purged. Only
Volumes marked as Purged will be considered to be converted to the
Recycled state if the {\bf Recycle} directive is set to {\bf yes}.

\label{RecycleOldest}
\item [Recycle Oldest Volume = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Recycle Oldest Volume}
\index[dir]{Directive!Recycle Oldest Volume}
This directive instructs the Director to search for the oldest used
Volume in the Pool when another Volume is requested by the Storage
daemon and none are available.  The catalog is then {\bf pruned}
respecting the retention periods of all Files and Jobs written to this
Volume.  If all Jobs are pruned (i.e. the volume is Purged), then the
Volume is recycled and will be used as the next Volume to be written.
This directive respects any Job, File, or Volume retention periods that
you may have specified, and as such it is {\bf much} better to use this
directive than the Purge Oldest Volume.

This directive can be useful if you have a fixed number of Volumes in the
Pool and you want to cycle through them and you have specified the correct
retention periods.

However, if you use this directive and have only one
Volume in the Pool, you will immediately recycle your Volume if you fill
it and Bareos needs another one. Thus your backup will be totally invalid.
Please use this directive with care. The default is {\bf no}.

\label{RecycleCurrent}
\item [Recycle Current Volume = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Recycle Current Volume}
\index[dir]{Directive!Recycle Current Volume}
If Bareos needs a new Volume, this directive instructs Bareos to Prune
the volume respecting the Job and File retention periods.  If all Jobs
are pruned (i.e.  the volume is Purged), then the Volume is recycled and
will be used as the next Volume to be written.  This directive respects
any Job, File, or Volume retention periods that you may have specified,
and thus it is {\bf much} better to use it rather than the Purge Oldest
Volume directive.

This directive can be useful if you have: a fixed number of Volumes in
the Pool, you want to cycle through them, and you have specified
retention periods that prune Volumes before you have cycled through the
Volume in the Pool.

However, if you use this directive and have only one Volume in the Pool,
you will immediately recycle your Volume if you fill it and Bareos needs
another one.  Thus your backup will be totally invalid.  Please use this
directive with care.  The default is {\bf no}.

\label{PurgeOldest}
\item [Purge Oldest Volume = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Purge Oldest Volume}
\index[dir]{Directive!Purge Oldest Volume}
This directive instructs the Director to search for the oldest used
Volume in the Pool when another Volume is requested by the Storage
daemon and none are available.  The catalog is then {\bf purged}
irrespective of retention periods of all Files and Jobs written to this
Volume.  The Volume is then recycled and will be used as the next Volume
to be written.  This directive overrides any Job, File, or Volume
retention periods that you may have specified.

This directive can be useful if you have a fixed number of Volumes in
the Pool and you want to cycle through them and reusing the oldest one
when all Volumes are full, but you don't want to worry about setting
proper retention periods.  However, by using this option you risk losing
valuable data.

Please be aware that {\bf Purge Oldest Volume} disregards all retention
periods. If you have only a single Volume defined and you turn this
variable on, that Volume will always be immediately overwritten when it
fills!  So at a minimum, ensure that you have a decent number of Volumes
in your Pool before running any jobs.  If you want retention periods to
apply do not use this directive.  To specify a retention period, use the
{\bf Volume Retention} directive (see above).

We {\bf highly} recommend against using this directive, because it is
sure that some day, Bareos will recycle a Volume that contains current
data.  The default is {\bf no}.

\item [File Retention = {\textless}time-period-specification{\textgreater}] \hfill \\
\index[dir]{File Retention}
\index[dir]{Directive!File Retention}
The File Retention directive defines the length of time that  Bareos will
keep File records in the Catalog database after the End time of the
Job corresponding to the File records.

This directive takes precedence over Client directives of the same name. For
example, you can decide to increase Retention times for Archive or OffSite
Pool.

Note, this affects only records in the catalog database. It does not affect
your archive backups.

For more information see Client documentation about
\ilink{FileRetention}{FileRetention}

\item [Job Retention = {\textless}time-period-specification{\textgreater}] \hfill \\
\index[dir]{Job Retention}
\index[dir]{Directive!Job Retention}

The Job Retention directive defines the length of time that Bareos will keep
Job records in the Catalog database after the Job End time.  As with the
other retention periods, this affects only records in the catalog and not
data in your archive backup.

This directive takes precedence over Client directives of the same name.
For example, you can decide to increase Retention times for Archive or
OffSite Pool.

For more information see Client side documentation
\ilink{JobRetention}{JobRetention}

\item [Cleaning Prefix = {\textless}string{\textgreater}] \hfill \\
\index[dir]{Cleaning Prefix}
\index[dir]{Directive!Cleaning Prefix}
This directive defines a prefix string, which if it matches the
beginning of a Volume name during labeling of a Volume, the Volume will
be defined with the VolStatus set to {\bf Cleaning} and thus Bareos will
never attempt to use this tape.  This is primarily for use with
autochangers that accept barcodes where the convention is that barcodes
beginning with {\bf CLN} are treated as cleaning tapes.

The default value for this directive is consequently set to {\bf CLN}, so
that in most cases the cleaning tapes are automatically recognized without
configuration.
If you use another prefix for your cleaning tapes, you can set this directive
accordingly.

\label{Label}
\item [Label Format = {\textless}format{\textgreater}] \hfill \\
\index[dir]{Label Format}
\index[dir]{Directive!Label Format}
This directive specifies the format of the labels contained in this
pool.  The format directive is used as a sort of template to create new
Volume names during automatic Volume labeling.

The {\bf format} should be specified in double quotes, and consists of
letters, numbers and the special characters hyphen ({\bf -}), underscore
({\bf \_}), colon ({\bf :}), and period ({\bf .}), which are the legal
characters for a Volume name.  The {\bf format} should be enclosed in
double quotes (").

In addition, the format may contain a number of variable expansion
characters which will be expanded by a complex algorithm allowing you to
create Volume names of many different formats.  In all cases, the
expansion process must resolve to the set of characters noted above that
are legal Volume names.  Generally, these variable expansion characters
begin with a dollar sign ({\bf \$}) or a left bracket ({\bf [}).  If you
specify variable expansion characters, you should always enclose the
format with double quote characters ({\bf "}).  For more details on
variable expansion, please see the \ilink{Variable
Expansion}{VarsChapter} Chapter of this manual.

If no variable expansion characters are found in the string, the Volume
name will be formed from the {\bf format} string appended with the
a unique number that increases.  If you do not remove volumes from the
pool, this number should be the number of volumes plus one, but this
is not guaranteed. The unique number will be edited as four
digits with leading zeros.  For example, with a {\bf Label Format =
"File-"}, the first volumes will be named {\bf File-0001}, {\bf
File-0002}, ...

With the exception of Job specific variables, you can test your {\bf
LabelFormat} by using the \ilink{var command}{var} the Console Chapter
of this manual.

In almost all cases, you should enclose the format specification (part
after the equal sign) in double quotes.  Please note that this directive
is deprecated and is replaced in version 1.37 and greater with a Python
script for creating volume names.
\end{description}

In order for a Pool to be used during a Backup Job, the Pool must have at
least one Volume associated with it.  Volumes are created for a Pool using
the {\bf label} or the {\bf add} commands in the {\bf Bareos Console},
program.  In addition to adding Volumes to the Pool (i.e.  putting the
Volume names in the Catalog database), the physical Volume must be labeled
with a valid Bareos software volume label before {\bf Bareos} will accept
the Volume.  This will be automatically done if you use the {\bf label}
command.  Bareos can automatically label Volumes if instructed to do so,
but this feature is not yet fully implemented.

The following is an example of a valid Pool resource definition:

\footnotesize
\begin{verbatim}

Pool {
  Name = Default
  Pool Type = Backup
}
\end{verbatim}
\normalsize

\subsection{Scratch Pool}
\label{TheScratchPool}
\index[general]{Scratch Pool}
\index[general]{Pool!Scratch}

In general, you can give your Pools any name you wish, but there is one
important restriction: the Pool named {\bf Scratch}, if it exists behaves
like a scratch pool of Volumes in that when Bareos needs a new Volume for
writing and it cannot find one, it will look in the Scratch pool, and if
it finds an available Volume, it will move it out of the Scratch pool into
the Pool currently being used by the job.

\section{Catalog Resource}
\label{CatalogResource}
\index[general]{Resource!Catalog}
\index[general]{Catalog Resource}

The Catalog Resource defines what catalog to use for the current job.
Currently, Bareos can only handle a single database server (SQLite, MySQL,
PostgreSQL) that is defined when configuring {\bf Bareos}.  However, there
may be as many Catalogs (databases) defined as you wish.  For example, you
may want each Client to have its own Catalog database, or you may want
backup jobs to use one database and verify or restore jobs to use another
database.

Since SQLite is compiled in, it always runs on the same machine
as the Director and the database must be directly accessible (mounted) from
the Director.  However, since both MySQL and PostgreSQL are networked
databases, they may reside either on the same machine as the Director
or on a different machine on the network.  See below for more details.


\begin{description}

\item [Catalog]
\index[dir]{Catalog}
\index[dir]{Directive!Catalog}
Start of the Catalog resource.  At least one Catalog resource must be
defined.

\item [Name = {\textless}name{\textgreater}] \hfill \\
\index[dir]{Name}
\index[dir]{Directive!Name}
The name of the Catalog.  No necessary relation to the database server
name.  This name will be specified in the Client resource directive
indicating that all catalog data for that Client is maintained in this
Catalog.  This directive is required.

\directive{dir}{dbdriver}{postgresql {\textbar} mysql {\textbar} sqlite}{required}{}{}
Selects the database type to use.

\directive{dir}{dbname}{name}{required}{}{}
This specifies the name of the database.  If you use multiple catalogs
(databases), you specify which one here.  If you are using an external
database server rather than the internal one, you must specify a name
that is known to the server (i.e.  you explicitly created the Bareos
tables using this name).

\directive{dir}{dbuser}{user}{required}{}{}
This specifies what user name to use to log into the database.

\directive{dir}{dbpassword}{password}{required}{}{}
This specifies the password to use when logging into the database.


\item [DB Socket = {\textless}socket-name{\textgreater}] \hfill \\
\index[dir]{DB Socket}
\index[dir]{Directive!DB Socket}
This is the name of  a socket to use on the local host to connect to the
database. This directive is used only by MySQL and is ignored by  SQLite.
Normally, if neither {\bf DB Socket} or {\bf DB Address}  are specified, MySQL
will use the default socket. If the DB Socket is specified, the
MySQL server must reside on the same machine as the Director.

\item [DB Address = {\textless}address{\textgreater}] \hfill \\
\index[dir]{DB Address}
\index[dir]{Directive!DB Address}
This is the host address  of the database server. Normally, you would specify
this instead  of {\bf DB Socket} if the database server is on another machine.
In that case, you will also specify {\bf DB Port}. This directive  is used
only by MySQL and PostgreSQL and is ignored by SQLite if provided.
This directive is optional.

\item [DB Port = {\textless}port{\textgreater}] \hfill \\
\index[dir]{DB Port}
\index[dir]{Directive!DB Port}
This defines the port to  be used in conjunction with {\bf DB Address} to
access the  database if it is on another machine. This directive is used  only
by MySQL and PostgreSQL and is ignored by SQLite if provided.  This
directive is optional.

%% \item [Multiple Connections = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
%% \index[dir]{Multiple Connections}
%% \index[dir]{Directive!Multiple Connections}
%% By default, this  directive is set to no. In that case, each job that uses the
%% same Catalog will use a single connection to the catalog. It will  be shared,
%% and Bareos will allow only one Job at a time to  communicate. If you set this
%% directive to yes, Bareos will  permit multiple connections to the database,
%% and the database  must be multi-thread capable. For SQLite and PostgreSQL,
%% this is  no problem. For MySQL, you must be *very* careful to have the
%% multi-thread version of the client library loaded on your system.  When this
%% directive is set yes, each Job will have a separate  connection to the
%% database, and the database will control the  interaction between the different
%% Jobs. This can significantly  speed up the database operations if you are
%% running multiple  simultaneous jobs. In addition, for SQLite and PostgreSQL,
%% Bareos  will automatically enable transactions. This can significantly  speed
%% up insertion of attributes in the database either for  a single Job or
%% multiple simultaneous Jobs.

%% This directive has not been tested. Please test carefully  before running it
%% in production and report back your results.

\item [Disable Batch Insert = {\textless}yes{\textbar}no{\textgreater}] \hfill \\
\index[dir]{Disable Batch Insert}
\index[dir]{Directive!Disable Batch Insert}
This directive allows you to override at runtime if the Batch insert should
be enabled or disabled. Normally this is determined by querying the database
library if it is thread-safe. If you think that disabling Batch insert will make
your backup run faster you may disable it using this option and set it to {\bf Yes}

\item [Min Connections = {\textless}Number{\textgreater}] \hfill \\
\index[dir]{Min Connections}
\index[dir]{Directive!Min Connections}
This directive is used by the experimental database pooling functionality. Only use
this for non production sites. This sets the minimum number of connections to a
database to keep in this database pool.

\item [Max Connections = {\textless}Number{\textgreater}] \hfill \\
\index[dir]{Max Connections}
\index[dir]{Directive!Max Connections}
This directive is used by the experimental database pooling functionality. Only use
this for non production sites. This sets the maximum number of connections to a
database to keep in this database pool.

\item [Inc Connections = {\textless}Number{\textgreater}] \hfill \\
\index[dir]{}
\index[dir]{Directive!}
This directive is used by the experimental database pooling functionality. Only use
this for non production sites. This sets the number of connections to add to a
database pool when not enough connections are available on the pool anymore.

\item [Idle Timeout = {\textless}Number{\textgreater}] \hfill \\
\index[dir]{Idle Timeout}
\index[dir]{Directive!Idle Timeout}
This directive is used by the experimental database pooling functionality. Only use
this for non production sites. This sets the idle time after which a database pool
should be shrinked.

\item [Validate Timeout = {\textless}Number{\textgreater}] \hfill \\
\index[dir]Validate Timeout{}
\index[dir]{Directive!Validate Timeout}
This directive is used by the experimental database pooling functionality. Only use
this for non production sites. This sets the validation timeout after which the
database connection is polled to see if its still alive.
\end{description}

The following is an example of a valid Catalog resource definition:

\footnotesize
\begin{verbatim}
Catalog
{
  Name = SQLite
  dbdriver = sqlite
  dbname = bareos;
  dbuser = bareos;
  dbpassword = ""                       # no password = no security
}
\end{verbatim}
\normalsize

or for a Catalog on another machine:

\footnotesize
\begin{verbatim}
Catalog
{
  Name = MySQL
  db Driver = mysql
  db Name = bareos
  db User = bareos
  db Password = ""
  db Address = remote.acme.com
  db Port = 1234
}
\end{verbatim}
\normalsize

\section{Messages Resource}
\label{MessagesResource2}
\index[general]{Resource!Messages}
\index[general]{Messages Resource}

For the details of the Messages Resource, please see the
\ilink{Messages Resource Chapter}{MessagesChapter} of this
manual.

\section{Console Resource}
\label{ConsoleResource1}
\index[general]{Console Resource}
\index[general]{Resource!Console}

There are three different kinds of consoles, which the administrator or
user can use to interact with the Director. These three kinds of consoles
comprise three different security levels.

\begin{itemize}
\item The first console type is an {\bf anonymous} or {\bf default}  console,
which has full privileges.  There is no console resource necessary for
this type since the password is specified in the Director's resource and
consequently such consoles do not have a name as defined on a {\bf Name
=} directive.  Typically you
would use it only for  administrators.

\item The second type of console, is a
"named" console defined within a Console resource in both the Director's
configuration file and in the Console's configuration file.  Both the
names and the passwords in these two entries must match much as is the
case for Client programs.

This second type of console begins with absolutely no privileges except
those explicitly specified in the Director's Console resource.  Thus you
can have multiple Consoles with different names and passwords, sort of
like multiple users, each with different privileges.  As a default,
these consoles can do absolutely nothing -- no commands whatsoever.  You
give them privileges or rather access to commands and resources by
specifying access control lists in the Director's Console resource.  The
ACLs are specified by a directive followed by a list of access names.
Examples of this are shown below.

\item The third type of console is similar to the above mentioned  one in that
it requires a Console resource definition in both the Director and the
Console.  In addition, if the console name, provided on the {\bf Name =}
directive, is the same as a Client name, that console is permitted to
use the {\bf SetIP} command to change the Address directive in the
Director's client resource to the IP address of the Console.  This
permits portables or other machines using DHCP (non-fixed IP addresses)
to "notify" the Director of their current IP address.
\end{itemize}

The Console resource is optional and need not be specified. The following
directives are permitted within the Director's configuration resource:

\begin{description}

\item [Name = {\textless}name{\textgreater}] \hfill \\
\index[dir]{Name}
\index[dir]{Directive!Name}
The name of the console. This  name must match the name specified in the
Console's configuration  resource (much as is the case with Client
definitions).

\item [Password = {\textless}password{\textgreater}] \hfill \\
\index[dir]{Password}
\index[dir]{Directive!Password}
Specifies the password that must be supplied for a named Bareos Console
to be authorized.  The same password must appear in the {\bf Console}
resource of the Console configuration file.  For added security, the
password is never actually passed across the network but rather a
challenge response hash code created with the password.  This directive
is required.

The password is plain text.  It is not generated through any special
process.  However, it is preferable for security reasons to choose
random text.

\item [JobACL = {\textless}name-list{\textgreater}] \hfill \\
\index[dir]{JobACL}
\index[dir]{Directive!JobACL}
This directive is used to specify a list of Job resource names that can
be accessed by the console.  Without this directive, the console cannot
access any of the Director's Job resources.  Multiple Job resource names
may be specified by separating them with commas, and/or by specifying
multiple JobACL directives.  For example, the directive may be specified
as:

\footnotesize
\begin{verbatim}
 JobACL = "Backup client 1", "Backup client 2"
 JobACL = "RestoreFiles"

\end{verbatim}
\normalsize

With the above specification, the console can access the Director's  resources
for the four jobs named on the JobACL directives,  but for no others.

\item [ClientACL = {\textless}name-list{\textgreater}] \hfill \\
\index[dir]{ClientACL}
\index[dir]{Directive!ClientACL}
This directive is used to  specify a list of Client resource names that can
be
accessed by  the console.

\item [StorageACL = {\textless}name-list{\textgreater}] \hfill \\
\index[dir]{StorageACL}
\index[dir]{Directive!StorageACL}
This directive is used to  specify a list of Storage resource names that can
be accessed by  the console.

\item [ScheduleACL = {\textless}name-list{\textgreater}] \hfill \\
\index[dir]{ScheduleACL}
\index[dir]{Directive!ScheduleACL}
This directive is used to  specify a list of Schedule resource names that can
be accessed by the console.

\item [PoolACL = {\textless}name-list{\textgreater}] \hfill \\
\index[dir]{PoolACL}
\index[dir]{Directive!PoolACL}
This directive is used to  specify a list of Pool resource names that can be
accessed by the console.

\item [FileSetACL = {\textless}name-list{\textgreater}] \hfill \\
\index[dir]{FileSetACL}
\index[dir]{Directive!FileSetACL}
This directive is used to specify a list of FileSet resource names that
can be accessed by the console.

\item [CatalogACL = {\textless}name-list{\textgreater}] \hfill \\
\index[dir]{CatalogACL}
\index[dir]{Directive!CatalogACL}
This directive is used to specify a list of Catalog resource names that
can be accessed by the console.

\item [CommandACL = {\textless}name-list{\textgreater}] \hfill \\
\index[dir]{CommandACL}
\index[dir]{Directive!CommandACL}
This directive is used to specify a list of of console commands that can
be executed by the console.

\item [WhereACL = {\textless}string{\textgreater}] \hfill \\
\index[dir]{WhereACL}
\index[dir]{Directive!WhereACL}
This directive permits you to specify where a restricted console
can restore files. If this directive is not specified, only the
default restore location is permitted (normally {\bf
/tmp/bareos-restores}. If {\bf *all*} is specified any path the
user enters will be accepted (not very secure), any other
value specified (there may be multiple WhereACL directives) will
restrict the user to use that path. For example, on a Unix system,
if you specify "/", the file will be restored to the original
location.  This directive is untested.

\end{description}

Aside from Director resource names and console command names, the special
keyword {\bf *all*} can be specified in any of the above access control lists.
When this keyword is present, any resource or command name (which ever is
appropriate) will be accepted. For an example configuration file, please see
the \ilink{Console Configuration}{ConsoleConfChapter} chapter of this manual.

\section{Counter Resource}
\label{CounterResource}
\index[general]{Resource!Counter}
\index[general]{Counter Resource}

The Counter Resource defines a counter variable that can be accessed by
variable expansion used for creating Volume labels with the {\bf LabelFormat}
directive. See the
\ilink{LabelFormat}{Label} directive in this chapter for more
details.

\begin{description}

\item [Counter]
\index[dir]{Counter}
\index[dir]{Directive!Counter}
Start of the Counter resource.  Counter directives are optional.

\item [Name = {\textless}name{\textgreater}] \hfill \\
\index[dir]{Name}
\index[dir]{Directive!Name}
The name of the Counter.  This is the name you will use in the variable
expansion  to reference the counter value.

\item [Minimum = {\textless}integer{\textgreater}] \hfill \\
\index[dir]{Minimum}
\index[dir]{Directive!Minimum}
This specifies the minimum  value that the counter can have. It also becomes
the default.  If not supplied, zero is assumed.

\item [Maximum = {\textless}integer{\textgreater}] \hfill \\
\index[dir]{Maximum}
\index[dir]{Directive!Maximum}
\index[dir]{Directive!Maximum}
This is the maximum value  value that the counter can have. If not specified
or set to  zero, the counter can have a maximum value of 2,147,483,648  (2 to
the 31 power). When the counter is incremented past  this value, it is reset
to the Minimum.

%\item [*WrapCounter = {\textless}counter-name{\textgreater}] \hfill \\
%\index[dir]{*WrapCounter}
%\index[dir]{Directive!*WrapCounter}
%If this value  is specified, when the counter is incremented past the
%maximum
%and thus reset to the minimum, the counter specified on the  {\bf WrapCounter}
%is incremented. (This is not currently  implemented).

\item [Catalog = {\textless}catalog-name{\textgreater}] \hfill \\
\index[dir]{Catalog}
\index[dir]{Directive!Catalog}
If this directive is  specified, the counter and its values will be saved in
the specified catalog. If this directive is not present, the  counter will be
redefined each time that Bareos is started.
\end{description}

\section{Example Director Configuration File}
\label{SampleDirectorConfiguration}
\index[general]{Configuration!Director!Example}
\index[dir]{Configuration File Example}

See below an example of a full Director configuration file:

{
\footnotesize
\verbatiminput{bareos-dir.conf.in}
}
